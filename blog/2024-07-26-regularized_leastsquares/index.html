<!DOCTYPE html>
<html lang="en" dir="ltr"><head>
  
                           
     


<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.128.0">
<title>Least squares regression: Part 2 | Cecina Babich Morrow</title>


<meta property="twitter:site" content="@babichmorrowc">
<meta property="twitter:creator" content="@babichmorrowc">







  
    
  
<meta name="description" content="A little more least squares regression.">


<meta property="og:site_name" content="Cecina Babich Morrow">
<meta property="og:title" content="Least squares regression: Part 2 | Cecina Babich Morrow">
<meta property="og:description" content="A little more least squares regression." />
<meta property="og:type" content="page" />
<meta property="og:url" content="https://babichmorrowc.github.io/blog/2024-07-26-regularized_leastsquares/" />
<meta property="og:locale" content="en">




    
        <meta property="og:image" content="https://babichmorrowc.github.io/blog/2024-07-26-regularized_leastsquares/featured.jpg" >
        <meta property="twitter:card" content="summary_large_image">
        <meta name="twitter:image" content="https://babichmorrowc.github.io/blog/2024-07-26-regularized_leastsquares/featured.jpg" >
    
    
  
  <meta itemprop="name" content="Least squares regression: Part 2">
  <meta itemprop="description" content="Feature transforms, overfitting, and cross-validation.">
  <meta itemprop="datePublished" content="2024-07-26T00:00:00+00:00">
  <meta itemprop="dateModified" content="2024-07-26T00:00:00+00:00">
  <meta itemprop="wordCount" content="2007">
  <meta itemprop="image" content="https://babichmorrowc.github.io/blog/2024-07-26-regularized_leastsquares/featured.jpg">
  <meta itemprop="keywords" content="Statistics,R">
  
  
    
      <script async src="https://www.googletagmanager.com/gtag/js?id=130807002"></script>
      <script>
        var doNotTrack = false;
        if ( true ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', '130807002');
        }
      </script>
    
  


  <!--[if IE]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  <link rel="shortcut icon" href="/img/favicon_io/favicon.ico" type="image/x-icon">
  <link rel="icon" href="/img/favicon_io/favicon.ico" type="image/x-icon">
  
  
  <link rel="stylesheet" href="/style.main.min.efe606a4a38248e49cb083e10d55824649e30549b2438e91c5513e6ccbbb543d.css" integrity="sha256-7&#43;YGpKOCSOScsIPhDVWCRknjBUmyQ46RxVE&#43;bMu7VD0=" media="screen">
  
  
  <script src="/panelset.min.ed1ac24b6e16f4e2481e3d1d098ae66f5bc77438aef619e6e266d8ac5b00dc72.js" type="text/javascript"></script>
  
  
  <script src="/main.min.8775cb64fbeada525af486d0e6a80a9042ba0c7a7958fc22c03e42d5abc3fe1f.js" type="text/javascript"></script>
</head>
<body>
      <div class="grid-container">
<header class="site-header pt4 pb2 mb4 bb b--transparent ph5 headroom z-max" role="banner">
  <nav class="site-nav db dt-l w-100" role="navigation">
    <a class="site-brand db dtc-l v-mid link no-underline w-100 w-33-l tc tl-l" href="https://babichmorrowc.github.io/" title="Home">
      <img src="/img/favicon_io/favicon.ico" class="dib db-l h2 w-auto" alt="Cecina Babich Morrow">
    </a>
    <div class="site-links db dtc-l v-mid w-100 w-47-l tc tr-l mt3 mt0-l ttu tracked">
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/about/" title="About Cecina">About</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 active" href="/blog/" title="Blog">Blog</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/project/" title="Project Portfolio">Projects</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/talk/" title="Talks">Talks</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/publication/" title="Publications">Publications</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/contact/" title="Contact">Contact</a>
      
      
    </div>
  </nav>
</header>

<main class="page-main pa4" role="main">
  <section class="page-content mw7 center">
    <article class="post-content pa0 ph4-l">
      <header class="post-header">
        <h1 class="f1 lh-solid measure-narrow mb3 fw4">Least squares regression: Part 2</h1>
        <h4 class="f4 mt0 mb4 lh-title measure">A little more least squares regression.</h4>
        <p class="f6 measure lh-copy mv1">By Cecina Babich Morrow in <a href="https://babichmorrowc.github.io/categories/statistics">statistics</a>  <a href="https://babichmorrowc.github.io/categories/r">R</a> </p>
        <p class="f7 db mv0 ttu">July 26, 2024</p>

      

      </header>
      <section class="post-body pt5 pb4">
        



<h2 id="inspiration-for-this-post">Inspiration for this post
  <a href="#inspiration-for-this-post"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>Following up on 
<a href="https://babichmorrowc.github.io/blog/2024-07-02-leastsquares-regression/" target="_blank" rel="noopener">my last post</a> about least-squares regression, here&rsquo;s part 2, where we cover feature transformation, the dangers of over-fitting, and a possible solution in the form of cross-validation. As before, most of the material I&rsquo;m showing comes from a class I took with Dr. Song Liu.</p>




<h2 id="feature-transformation">Feature transformation
  <a href="#feature-transformation"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>Since we are often living in a non-linear world, linear least-squares
might not cut it. To address this issue, we can apply a feature
transform to the input variables before we apply least squares. We
define a new prediction function
<code>\(f'(\textbf{x};\textbf{w}) := \langle\textbf{w}_1, \phi(\textbf{x})\rangle + w_0\)</code>
using a feature transform
<code>\(\phi(\textbf{x}): \mathbb{R}^d \rightarrow \mathbb{R}^b\)</code> with degree
<code>\(b\)</code>. Then minimizing the sum of squared errors yields
<code>\(\textbf{w}_{LS}=(\phi(\textbf{X})\phi(\textbf{X})^\top)^{-1}\phi(\textbf{X})\textbf{y}^\top\)</code>.
If <code>\(\phi(\textbf{X})\)</code> is symmetric and invertible, then we can simplify
further:</p>
<p><strong>Theorem 1 (Least squares with a feature transform):</strong> If <code>\(\phi(\textbf{X})\)</code> is symmetric and invertible, then <code>\(\textbf{w}_{LS}=[\phi(\textbf{X})]^{-1}\textbf{y}^\top\)</code>.</p>
<p><strong>Proof:</strong></p>
<p>$$
<code>\begin{align*} \textbf{w}_{LS} &amp;= [\phi(\textbf{X})\phi(\textbf{X})^\top]^{-1}\phi(\textbf{X})\textbf{y}^\top \\ &amp;= [\phi(\textbf{X})^\top]^{-1}[\phi(\textbf{X})]^{-1}\phi(\textbf{X})\textbf{y}^\top \\ &amp;= [\phi(\textbf{X})^\top]^{-1}\textbf{y}^\top \\ &amp;= [\phi(\textbf{X})]^{-1}\textbf{y}^\top \end{align*}</code>
$$</p>
<p>â– </p>




<h3 id="r-implementation">R implementation
  <a href="#r-implementation"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>In 
<a href="https://babichmorrowc.github.io/blog/2024-07-02-leastsquares-regression/" target="_blank" rel="noopener">the last post</a>, we created two functions: <code>least_squares_solver()</code> to find <code>\(\textbf{w}_{LS}\)</code> and <code>least_squares_predict()</code> to make predictions using the coefficients we calculate on a new dataset. We&rsquo;re going to need to extend these functions to handle the extensions we&rsquo;re making. First, we&rsquo;ll allow our functions to accommodate polynomial feature transformation of the form <code>\(\phi(\textbf{x}) = [\textbf{x}, \textbf{x}^2, \textbf{x}^3, ..., \textbf{x}^b]^\top\)</code>:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>least_squares_solver_ft <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#000;font-weight:bold">function</span>(input_variables,
</span></span><span style="display:flex;"><span>                                    output_variable, 
</span></span><span style="display:flex;"><span>                                    b <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">1</span>) {
</span></span><span style="display:flex;"><span>  <span style="color:#998;font-style:italic"># Set up variables</span>
</span></span><span style="display:flex;"><span>  y <span style="color:#000;font-weight:bold">&lt;-</span> output_variable
</span></span><span style="display:flex;"><span>  phi_X <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">t</span>(<span style="color:#900;font-weight:bold">as.matrix</span>(input_variables^b)) <span style="color:#998;font-style:italic"># convert each x_i into column vector</span>
</span></span><span style="display:flex;"><span>  power <span style="color:#000;font-weight:bold">&lt;-</span> b <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">1</span>
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">while</span>(power <span style="color:#000;font-weight:bold">&gt;</span> <span style="color:#099">0</span>) {
</span></span><span style="display:flex;"><span>    phi_X <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">rbind</span>(phi_X, <span style="color:#900;font-weight:bold">t</span>(<span style="color:#900;font-weight:bold">as.matrix</span>(input_variables^power)))
</span></span><span style="display:flex;"><span>    power <span style="color:#000;font-weight:bold">&lt;-</span> power <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">1</span>
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>  <span style="color:#998;font-style:italic"># Add intercept</span>
</span></span><span style="display:flex;"><span>  phi_X <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">rbind</span>(phi_X, <span style="color:#900;font-weight:bold">rep</span>(<span style="color:#099">1</span>, <span style="color:#900;font-weight:bold">ncol</span>(phi_X)))
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>  <span style="color:#998;font-style:italic"># Compute w_LS</span>
</span></span><span style="display:flex;"><span>  w_LS <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">solve</span>(phi_X <span style="color:#000;font-weight:bold">%*%</span> <span style="color:#900;font-weight:bold">t</span>(phi_X)) <span style="color:#000;font-weight:bold">%*%</span> phi_X <span style="color:#000;font-weight:bold">%*%</span> <span style="color:#900;font-weight:bold">as.matrix</span>(y)
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">return</span>(w_LS)
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Function to apply coefficients to a new set of data</span>
</span></span><span style="display:flex;"><span>least_squares_predict_ft <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#000;font-weight:bold">function</span>(input_variables,
</span></span><span style="display:flex;"><span>                                     coefficients,
</span></span><span style="display:flex;"><span>                                     b <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">1</span>) {
</span></span><span style="display:flex;"><span>  <span style="color:#998;font-style:italic"># Set up variables</span>
</span></span><span style="display:flex;"><span>  phi_X <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">t</span>(<span style="color:#900;font-weight:bold">as.matrix</span>(input_variables^b)) <span style="color:#998;font-style:italic"># convert each x_i into column vector</span>
</span></span><span style="display:flex;"><span>  power <span style="color:#000;font-weight:bold">&lt;-</span> b <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">1</span>
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">while</span>(power <span style="color:#000;font-weight:bold">&gt;</span> <span style="color:#099">0</span>) {
</span></span><span style="display:flex;"><span>    phi_X <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">rbind</span>(phi_X, <span style="color:#900;font-weight:bold">t</span>(<span style="color:#900;font-weight:bold">as.matrix</span>(input_variables^power)))
</span></span><span style="display:flex;"><span>    power <span style="color:#000;font-weight:bold">&lt;-</span> power <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">1</span>
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>  <span style="color:#998;font-style:italic"># Add intercept</span>
</span></span><span style="display:flex;"><span>  phi_X <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">rbind</span>(phi_X, <span style="color:#900;font-weight:bold">rep</span>(<span style="color:#099">1</span>, <span style="color:#900;font-weight:bold">ncol</span>(phi_X)))
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>  preds <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">t</span>(<span style="color:#900;font-weight:bold">as.matrix</span>(coefficients)) <span style="color:#000;font-weight:bold">%*%</span> phi_X
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">return</span>(preds)
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Function to calculate sum of squared errors</span>
</span></span><span style="display:flex;"><span>sum_squared_errors <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#000;font-weight:bold">function</span>(prediction, actual) {
</span></span><span style="display:flex;"><span>  squared_error <span style="color:#000;font-weight:bold">&lt;-</span> (actual <span style="color:#000;font-weight:bold">-</span> prediction)^2
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">return</span>(<span style="color:#900;font-weight:bold">sum</span>(squared_error))
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>



<h2 id="overfitting">Overfitting
  <a href="#overfitting"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>By introducing more complex feature transformations, we encounter the
trade-off between model flexibility and the generalizability of our
predictions. As the degree <code>\(b\)</code> of our feature transformation function <code>\(\phi(\textbf{X})\)</code> increases, our prediction function becomes more and more flexible. we can quantify this problem, known as overfitting, by computing <code>\(\textbf{w}_{LS}\)</code> on one portion of our data and testing it out on a different part of the data to determine how generalizable our predictions are to unseen data. We split our dataset <code>\(D\)</code> at random into a training dataset <code>\(D_0\)</code> and a testing dataset <code>\(D_1\)</code> and fit the model on <code>\(D_0\)</code> alone.</p>
<blockquote>
<p><em>Assumption alert:</em> Note that this process depends on the assumption that our dataset contains IID pairs of input and output variables. This assumption does not hold for series data, for instance.</p>
</blockquote>
<p>This process allows us to calculate two measurements of how well our model is doing: (1) training error, how well our model predicts the data <code>\(D_0\)</code> on which it was trained, and (2) testing error, how well the model predicts data on which it was not trained, i.e. generalization. As <code>\(b\)</code> increases, training error continues to decrease as our model becomes increasingly flexible to the data on which it was trained. At first, testing error decreases while <code>\(b\)</code> increases, but past a certain value of <code>\(b\)</code>, testing error increases once again as our prediction function loses generalization, meaning that our model is overfitting.</p>




<h3 id="example">Example
  <a href="#example"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>To see this in action, let&rsquo;s try our new <code>least_squares_solver_ft()</code> function on a randomly generated dataset:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span><span style="color:#900;font-weight:bold">library</span>(ggplot2)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Generate data ----------------------------------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#900;font-weight:bold">set.seed</span>(<span style="color:#099">42</span>)
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># x_i uniformly generated (0,2)</span>
</span></span><span style="display:flex;"><span>x <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">runif</span>(<span style="color:#099">200</span>, min <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>, max <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">2</span>)
</span></span><span style="display:flex;"><span>epsilon <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">rnorm</span>(<span style="color:#099">200</span>, mean <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>, sd <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">24</span>)
</span></span><span style="display:flex;"><span>y <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">exp</span>(<span style="color:#099">3</span><span style="color:#000;font-weight:bold">*</span>x <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">1</span>) <span style="color:#000;font-weight:bold">+</span> epsilon
</span></span><span style="display:flex;"><span>generated_data <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">data.frame</span>(x_val <span style="color:#000;font-weight:bold">=</span> x, y_val <span style="color:#000;font-weight:bold">=</span> y)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Visualize</span>
</span></span><span style="display:flex;"><span><span style="color:#900;font-weight:bold">ggplot</span>(data <span style="color:#000;font-weight:bold">=</span> generated_data, <span style="color:#900;font-weight:bold">aes</span>(x <span style="color:#000;font-weight:bold">=</span> x_val, y <span style="color:#000;font-weight:bold">=</span> y_val)) <span style="color:#000;font-weight:bold">+</span>
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">geom_point</span>() <span style="color:#000;font-weight:bold">+</span>
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">theme_bw</span>()
</span></span></code></pre></div><img src="https://babichmorrowc.github.io/blog/2024-07-26-regularized_leastsquares/index_files/figure-html/unnamed-chunk-2-1.png" width="672" />
<p>We&rsquo;ll start with running a standard linear least squares, equivalent to a feature transform where <code>\(b = 1\)</code>:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Model with b = 1 -----------------------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Fit linear least squares on the training data</span>
</span></span><span style="display:flex;"><span>least_squares_1 <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">least_squares_solver_ft</span>(generated_data<span style="color:#000;font-weight:bold">$</span>x_val,
</span></span><span style="display:flex;"><span>                                           generated_data<span style="color:#000;font-weight:bold">$</span>y_val,
</span></span><span style="display:flex;"><span>                                           b <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Apply the model to the testing data</span>
</span></span><span style="display:flex;"><span>preds_1 <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">least_squares_predict_ft</span>(generated_data<span style="color:#000;font-weight:bold">$</span>x_val, least_squares_1)
</span></span><span style="display:flex;"><span>generated_data<span style="color:#000;font-weight:bold">$</span>preds_1 <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">as.vector</span>(preds_1)
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Calculate error</span>
</span></span><span style="display:flex;"><span>error_1 <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">sum_squared_errors</span>(preds_1, generated_data<span style="color:#000;font-weight:bold">$</span>y_val)
</span></span><span style="display:flex;"><span>error_1
</span></span></code></pre></div><pre tabindex="0"><code>## [1] 179048.1
</code></pre><p>We can visualize our results:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span><span style="color:#900;font-weight:bold">library</span>(ggplot2)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#900;font-weight:bold">ggplot</span>(data <span style="color:#000;font-weight:bold">=</span> generated_data) <span style="color:#000;font-weight:bold">+</span>
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">geom_point</span>(<span style="color:#900;font-weight:bold">aes</span>(x <span style="color:#000;font-weight:bold">=</span> x_val, y <span style="color:#000;font-weight:bold">=</span> y_val)) <span style="color:#000;font-weight:bold">+</span>
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">geom_line</span>(<span style="color:#900;font-weight:bold">aes</span>(x <span style="color:#000;font-weight:bold">=</span> x_val, y <span style="color:#000;font-weight:bold">=</span> preds_1), color <span style="color:#000;font-weight:bold">=</span> <span style="color:#d14">&#34;red&#34;</span>) <span style="color:#000;font-weight:bold">+</span>
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">theme_bw</span>()
</span></span></code></pre></div><img src="https://babichmorrowc.github.io/blog/2024-07-26-regularized_leastsquares/index_files/figure-html/unnamed-chunk-4-1.png" width="672" />
<p>We&rsquo;re clearly not capturing the non-linear pattern in the data very well. Let&rsquo;s try a very flexible feature transformation with <code>\(b = 9\)</code> to see if that addresses our problem:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Model with b = 9 -----------------------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Fit linear least squares on the training data</span>
</span></span><span style="display:flex;"><span>least_squares_9 <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">least_squares_solver_ft</span>(generated_data<span style="color:#000;font-weight:bold">$</span>x_val,
</span></span><span style="display:flex;"><span>                                           generated_data<span style="color:#000;font-weight:bold">$</span>y_val,
</span></span><span style="display:flex;"><span>                                           b <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">9</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Apply the model to the testing data</span>
</span></span><span style="display:flex;"><span>preds_9 <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">least_squares_predict_ft</span>(generated_data<span style="color:#000;font-weight:bold">$</span>x_val,
</span></span><span style="display:flex;"><span>                                    least_squares_9,
</span></span><span style="display:flex;"><span>                                    b <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">9</span>)
</span></span><span style="display:flex;"><span>generated_data<span style="color:#000;font-weight:bold">$</span>preds_9 <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">as.vector</span>(preds_9)
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Calculate error</span>
</span></span><span style="display:flex;"><span>error_9 <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">sum_squared_errors</span>(preds_9, generated_data<span style="color:#000;font-weight:bold">$</span>y_val)
</span></span><span style="display:flex;"><span>error_9
</span></span></code></pre></div><pre tabindex="0"><code>## [1] 103317.3
</code></pre><p>We have a much lower sum of squared errors.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span><span style="color:#900;font-weight:bold">ggplot</span>(data <span style="color:#000;font-weight:bold">=</span> generated_data) <span style="color:#000;font-weight:bold">+</span>
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">geom_point</span>(<span style="color:#900;font-weight:bold">aes</span>(x <span style="color:#000;font-weight:bold">=</span> x_val, y <span style="color:#000;font-weight:bold">=</span> y_val)) <span style="color:#000;font-weight:bold">+</span>
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">geom_line</span>(<span style="color:#900;font-weight:bold">aes</span>(x <span style="color:#000;font-weight:bold">=</span> x_val, y <span style="color:#000;font-weight:bold">=</span> preds_9), color <span style="color:#000;font-weight:bold">=</span> <span style="color:#d14">&#34;red&#34;</span>) <span style="color:#000;font-weight:bold">+</span>
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">theme_bw</span>()
</span></span></code></pre></div><img src="https://babichmorrowc.github.io/blog/2024-07-26-regularized_leastsquares/index_files/figure-html/unnamed-chunk-6-1.png" width="672" />
<p>We are seeing some wiggliness (a highly technical term) to our prediction line that we know doesn&rsquo;t reflect the underlying pattern in our data, however. This indicates overfitting, where our model is too tuned into the random variation in our sample and loses the ability to generalize to unseen data.</p>




<h3 id="benign-overfitting">Benign overfitting
  <a href="#benign-overfitting"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>Typically, we observe this pattern of increasing the complexity of the prediction function first yielding improvements in testing error, and then deteriorated performance as complexity increases past a certain point. This pattern is known as the bias-variance trade-off. As with most things, though, there can be exceptions to this general process.</p>
<p>Typically, we observe this pattern of increasing the complexity of the prediction function first yielding improvements in testing error, and then deteriorated performance as complexity increases past a certain point. This pattern is known as the bias-variance trade-off.</p>
<p>
<a href="https://www.pnas.org/doi/full/10.1073/pnas.1907378117" target="_blank" rel="noopener">Bartlett <em>et al.</em> (2020)</a> focused on the statistical problem of linear regression to determine whether benign overfitting can occur here as well, and if so, under what conditions. They examine the case where the data has infinite dimension and the prediction function fits the training data exactly. The latter occurs when there are more parameters than datapoints, which can yield multiple solutions which all minimize squared error, so they select the parameter with the smallest norm out of those which yield a training error of zero, a technique known as minimum norm estimation. The authors then characterize situations where the resulting prediction function will have low testing error as well, i.e. when benign overfitting occurs.</p>
<p>The authors find that in order for benign overfitting to occur, the covariance matrix <code>\(\Sigma\)</code> of the input variables needs to meet certain criteria. They define two notions of effective rank defined in terms of the eigenvalues of <code>\(\Sigma\)</code>: <code>\(r\)</code>, which splits <code>\(\Sigma\)</code> into large and small eigenvalues, and <code>\(R\)</code>, which is the effective rank of the subspace corresponding to the smallest eigenvalues. Using these two types of effective rank, they find that the minimum norm estimator has the highest accuracy when <code>\(\Sigma\)</code> has many non-zero eigenvalues compared to the sample size <code>\(n\)</code>, the sum of those eigenvalues is small compared to <code>\(n\)</code>, and there are many eigenvalues less than or equal to <code>\(\lambda_{k^*}\)</code>, the largest eigenvalue such that <code>\(r_{k^*}(\Sigma) \geq bn\)</code>. The eigenvalues of the covariance matrix need to decay sufficiently slowly, while still having a relatively low sum. In practice, this means that benign overfitting requires overparameterization, meaning that there are many low-variance directions in the parameter space, such that the covariance matrix has a heavy tail. When the data has infinite dimension, the decay rate of the eigenvalues must be within a much narrower range of values in order to cause benign overfitting. In a finite dimensional space, however, as long as the dimension of the space is growing faster than the sample size, benign overfitting is likely to occur.</p>




<h2 id="cross-validation">Cross-validation
  <a href="#cross-validation"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>We need some way to find the balance between flexibility and generalizability in our model. We have several potential options of how to apply the concept of training/testing splits. In the simplest method, we can fit <code>\(f_{LS}(b)\)</code> on <code>\(D_0\)</code>, compute testing error on <code>\(D_1\)</code>, and select the value of <code>\(b\)</code> that yields the lowest testing error. This approach effectively wastes the data in <code>\(D_1\)</code> for validation, however, and since the testing error is random, the selection of <code>\(b\)</code> may be random. To address these issues, we can use a method called cross-validation to find <code>\(b\)</code>. We split <code>\(D\)</code> into <code>\(k+1\)</code> disjoint subsets <code>\(D_0...D_k\)</code> (note that <code>\(k\)</code> can be as large as <code>\(n - 1\)</code>, which is known as leave-one-out validation). For <code>\(i\in{0,...,k}\)</code>, fit <code>\(f_{LS}^{(i)}(b)\)</code> on all subsets except <code>\(D_i\)</code> for all values of <code>\(b\)</code>, and then compute the testing error <code>\(E^{(i)}\)</code> using <code>\(D_i\)</code> as the testing data. Then select the value of <code>\(b\)</code> that minimizes the average testing error <code>\(\frac{\sum_i E^{(i)}}{k+1}\)</code>.</p>
<p>Cross-validation is not without its drawbacks, however. Not only does it depend on the IID assumption, but it also becomes very computationally expensive, especially as the dimensionality of the dataset increases. When <code>\(\textbf{x} \in \mathbb{R}^d\)</code>, then <code>\(\phi(\textbf{x}) \in \mathbb{R}^{db}\)</code>, meaning that <code>\(\textbf{w} \in \mathbb{R}^{db+1}\)</code> without any cross-dimensional polynomials. Adding cross-dimensional polynomials, which are useful when the output value depends on the interaction between multiple inputs, can increase the dimensionality of <code>\(\phi(\textbf{x})\)</code> all the way up to <code>\(\mathbb{R}^{db + {d \choose 2}}\)</code>. We can include combinations of terms all the way up to <code>\(d\)</code>-plets: <code>\({d \choose 1} + {d \choose 2} + ... + {d \choose d} = 2^d\)</code>. Since the sample size needs to match the output dimension of <code>\(\phi(\textbf{x})\)</code> at a minimum, the sample size has to grow exponentially with the dimension of <code>\(\textbf{x}\)</code>, as well as with the degree of the feature transform. Practically, we are typically limited by how much data we can realistically collect, as well as by the computing power needed to analyze it. This issue is known (somewhat dramatically) as the curse of dimensionality.</p>




<h3 id="r-implementation-1">R implementation
  <a href="#r-implementation-1"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>We can use 5-fold cross-validation to find the degree <code>\(b\)</code> that gives us the lowest sum of squared errors on our generated dataset:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Calculate CV error with different values of b ---------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Use 5-fold cross-validation</span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Randomly shuffle the data</span>
</span></span><span style="display:flex;"><span>generated_data <span style="color:#000;font-weight:bold">&lt;-</span>  generated_data<span style="color:#900;font-weight:bold">[sample</span>(<span style="color:#900;font-weight:bold">nrow</span>(generated_data)),]
</span></span><span style="display:flex;"><span>folds <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">cut</span>(<span style="color:#900;font-weight:bold">seq</span>(<span style="color:#099">1</span>,<span style="color:#900;font-weight:bold">nrow</span>(generated_data)), breaks <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">5</span>, labels <span style="color:#000;font-weight:bold">=</span> <span style="color:#000;font-weight:bold">FALSE</span>)
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Set up feature space</span>
</span></span><span style="display:flex;"><span>degrees <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#099">1</span><span style="color:#000;font-weight:bold">:</span><span style="color:#099">9</span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Empty dataframe to store cross-validation error</span>
</span></span><span style="display:flex;"><span>cross_val_results <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">data.frame</span>(b <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">numeric</span>(), cross_val_error <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">numeric</span>())
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Loop over all b values</span>
</span></span><span style="display:flex;"><span><span style="color:#000;font-weight:bold">for</span> (b <span style="color:#000;font-weight:bold">in</span> degrees) {
</span></span><span style="display:flex;"><span>  cv_error <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">c</span>()
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">for</span>(i <span style="color:#000;font-weight:bold">in</span> <span style="color:#099">1</span><span style="color:#000;font-weight:bold">:</span><span style="color:#099">5</span>) {
</span></span><span style="display:flex;"><span>    test_data <span style="color:#000;font-weight:bold">&lt;-</span> generated_data<span style="color:#900;font-weight:bold">[which</span>(folds <span style="color:#000;font-weight:bold">==</span> i),]
</span></span><span style="display:flex;"><span>    train_data <span style="color:#000;font-weight:bold">&lt;-</span> generated_data<span style="color:#900;font-weight:bold">[which</span>(folds <span style="color:#000;font-weight:bold">!=</span> i),]
</span></span><span style="display:flex;"><span>    fit_i <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">least_squares_solver_ft</span>(train_data<span style="color:#000;font-weight:bold">$</span>x_val,
</span></span><span style="display:flex;"><span>                                     train_data<span style="color:#000;font-weight:bold">$</span>y_val,
</span></span><span style="display:flex;"><span>                                     b <span style="color:#000;font-weight:bold">=</span> b)
</span></span><span style="display:flex;"><span>    pred_i <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">least_squares_predict_ft</span>(test_data<span style="color:#000;font-weight:bold">$</span>x_val, fit_i, b <span style="color:#000;font-weight:bold">=</span> b)
</span></span><span style="display:flex;"><span>    error_i <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">sum_squared_errors</span>(pred_i, test_data<span style="color:#000;font-weight:bold">$</span>y_val)
</span></span><span style="display:flex;"><span>    cv_error <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">c</span>(cv_error, error_i)
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>  overall_error <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">mean</span>(cv_error)
</span></span><span style="display:flex;"><span>  cross_val_results[b,] <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">c</span>(b, overall_error)
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>Let&rsquo;s see the &ldquo;sweet spot&rdquo; where we have the lowest cross-validation error:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span><span style="color:#900;font-weight:bold">ggplot</span>(data <span style="color:#000;font-weight:bold">=</span> cross_val_results, <span style="color:#900;font-weight:bold">aes</span>(x <span style="color:#000;font-weight:bold">=</span> b, y <span style="color:#000;font-weight:bold">=</span> cross_val_error)) <span style="color:#000;font-weight:bold">+</span>
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">geom_point</span>() <span style="color:#000;font-weight:bold">+</span>
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">scale_x_continuous</span>(breaks <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span><span style="color:#000;font-weight:bold">:</span><span style="color:#099">10</span>) <span style="color:#000;font-weight:bold">+</span>
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">labs</span>(x <span style="color:#000;font-weight:bold">=</span> <span style="color:#d14">&#34;b&#34;</span>, y <span style="color:#000;font-weight:bold">=</span> <span style="color:#d14">&#34;Cross-validation error&#34;</span>) <span style="color:#000;font-weight:bold">+</span>
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">theme_bw</span>()
</span></span></code></pre></div><img src="https://babichmorrowc.github.io/blog/2024-07-26-regularized_leastsquares/index_files/figure-html/unnamed-chunk-8-1.png" width="672" />
<p>We have the lowest cross-validation error at <code>\(b = 4\)</code>. Let&rsquo;s visualize what that looks like in terms of predictions on our whole dataset:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Model with b = 4 -----------------------------------------------------</span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Fit linear least squares on the training data</span>
</span></span><span style="display:flex;"><span>least_squares_4 <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">least_squares_solver_ft</span>(generated_data<span style="color:#000;font-weight:bold">$</span>x_val,
</span></span><span style="display:flex;"><span>                                           generated_data<span style="color:#000;font-weight:bold">$</span>y_val,
</span></span><span style="display:flex;"><span>                                           b <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">4</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Apply the model to the testing data</span>
</span></span><span style="display:flex;"><span>preds_4 <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">least_squares_predict_ft</span>(generated_data<span style="color:#000;font-weight:bold">$</span>x_val,
</span></span><span style="display:flex;"><span>                                    least_squares_4,
</span></span><span style="display:flex;"><span>                                    b <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">4</span>)
</span></span><span style="display:flex;"><span>generated_data<span style="color:#000;font-weight:bold">$</span>preds_4 <span style="color:#000;font-weight:bold">&lt;-</span> <span style="color:#900;font-weight:bold">as.vector</span>(preds_4)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Visualize</span>
</span></span><span style="display:flex;"><span><span style="color:#900;font-weight:bold">ggplot</span>(data <span style="color:#000;font-weight:bold">=</span> generated_data) <span style="color:#000;font-weight:bold">+</span>
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">geom_point</span>(<span style="color:#900;font-weight:bold">aes</span>(x <span style="color:#000;font-weight:bold">=</span> x_val, y <span style="color:#000;font-weight:bold">=</span> y_val)) <span style="color:#000;font-weight:bold">+</span>
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">geom_line</span>(<span style="color:#900;font-weight:bold">aes</span>(x <span style="color:#000;font-weight:bold">=</span> x_val, y <span style="color:#000;font-weight:bold">=</span> preds_4), color <span style="color:#000;font-weight:bold">=</span> <span style="color:#d14">&#34;red&#34;</span>) <span style="color:#000;font-weight:bold">+</span>
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">theme_bw</span>()
</span></span></code></pre></div><img src="https://babichmorrowc.github.io/blog/2024-07-26-regularized_leastsquares/index_files/figure-html/unnamed-chunk-9-1.png" width="672" />
<p>We have successfully de-wiggled our model!</p>




<h2 id="references">References
  <a href="#references"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>Bartlett, P.L., P.M. Long, G. Lugosi, and A. Tsigler. &ldquo;Benign overfitting in linear regression&rdquo;, PNAS, 2020.</p>

        
        <details closed class="f6 fw7 input-reset">
  <dl class="f6 lh-copy">
    <dt class="fw7">Posted on:</dt>
    <dd class="fw5 ml0">July 26, 2024</dd>
  </dl>
  <dl class="f6 lh-copy">
    <dt class="fw7">Length:</dt>
    <dd class="fw5 ml0">10 minute read, 2007 words</dd>
  </dl>
  
  <dl class="f6 lh-copy">
    <dt class="fw7">Categories:</dt>
    <dd class="fw5 ml0"> <a href="https://babichmorrowc.github.io/categories/statistics">statistics</a>  <a href="https://babichmorrowc.github.io/categories/r">R</a> </dd>
  </dl>
  
  
  
  <dl class="f6 lh-copy">
    <dt class="fw7">Tags:</dt>
    <dd class="fw5 ml0"> <a href="https://babichmorrowc.github.io/tags/statistics">statistics</a>  <a href="https://babichmorrowc.github.io/tags/r">R</a> </dd>
  </dl>
  
  <dl class="f6 lh-copy">
    <dt class="fw7">See Also:</dt>
    
    <dd class="fw5 ml0"><a href="/publication/bda_sensitivity/">From climate risk to action: Analysing adaptation decision robustness under uncertainty</a></dd>
    
    <dd class="fw5 ml0"><a href="/blog/2024-11-29-rsa/">Regional Sensitivity Analysis</a></dd>
    
    <dd class="fw5 ml0"><a href="/blog/2024-09-23-bias-variance/">Bias-variance decomposition</a></dd>
    
  </dl>
</details>

      </section>
      <footer class="post-footer">
        <div class="post-pagination dt w-100 mt4 mb2">
  
  
    <a class="prev dtc pr2 tl v-top fw6"
    href="https://babichmorrowc.github.io/blog/2024-08-17-how-brat-is-it/">&larr; brat and it&#39;s the same but it&#39;s a blog post so it&#39;s not</a>
  
  
  
    <a class="next dtc pl2 tr v-top fw6"
    href="https://babichmorrowc.github.io/blog/2024-07-02-leastsquares-regression/">Least squares regression: Part 1 &rarr;</a>
  
</div>

      </footer>
    </article>
    
      <div class="post-comments pa0 pa4-l mt4">
  
    <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "babichmorrowc-github-io" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
  
  
</div>

    
  </section>
</main>
<footer class="site-footer pv4 bt b--transparent ph5" role="contentinfo">
  <nav class="db dt-l w-100">
    <p class="site-copyright f7 db dtc-l v-mid w-100 w-33-l tc tl-l pv2 pv0-l mv0 lh-copy">
      &copy; 2025 Cecina Babich Morrow, Anywhere
      <span class="middot-divider"></span>
      Made with <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/hugo-apero/" rel="dct:source">Hugo ApÃ©ro</a></span>.
      <br />
      
Based on <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/formspree/blogophonic-hugo" rel="dct:source">Blogophonic</a></span> by <a xmlns:cc="http://creativecommons.org/ns#" href="https://formspree.io" property="cc:attributionName" rel="cc:attributionURL">Formspree</a>.
    </p>
    
    <div class="site-social-links db dtc-l v-mid w-100 w-33-l tc pv2 pv0-l mv0">
      <div class="social-icon-links" aria-hidden="true">
  
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="/contact/" title="envelope" >
      <i class="fas fa-envelope fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://github.com/babichmorrowc" title="github" target="_blank" rel="me noopener">
      <i class="fab fa-github fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://scholar.google.com/citations?user=20mEEooAAAAJ&amp;hl=en" title="google-scholar" target="_blank" rel="me noopener">
      <i class="fab fa-google-scholar fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://orcid.org/my-orcid?orcid=0000-0003-2188-1495" title="orcid" target="_blank" rel="me noopener">
      <i class="fab fa-orcid fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://x.com/babichmorrowc" title="x-twitter" target="_blank" rel="me noopener">
      <i class="fab fa-x-twitter fa-lg fa-fw"></i>
    </a>
  
</div>

    </div>
    
    <div class="site-links f6 db dtc-l v-mid w-100 w-67-l tc tr-l pv2 pv0-l mv0">
      
      <a class="dib pv1 ph2 link" href="/license/" title="License">License</a>
      
      <a class="dib pv1 ph2 link" href="/contact/" title="Contact form">Contact</a>
      
      <a class="dib pv1 ph2 link" href="/contributors/" title="Contributors">Contributors</a>
      
    </div>
  </nav>
  
    <script>

    var i, text, code, codes = document.getElementsByTagName('code');
    for (let i = 0; i < codes.length;) {
      code = codes[i];
      if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
        text = code.textContent;
        if (/^\$[^$]/.test(text) && /[^$]\$$/.test(text)) {
          text = text.replace(/^\$/, '\\(').replace(/\$$/, '\\)');
          code.textContent = text;
        }
        if (/^\\\((.|\s)+\\\)$/.test(text) ||
            /^\\\[(.|\s)+\\\]$/.test(text) ||
            /^\$(.|\s)+\$$/.test(text) ||
            /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
          code.outerHTML = code.innerHTML;  
          continue;
        }
      }
      i++;
    }
</script>

  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css" integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js" integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>



    
  
  
</footer>

      </div>
    </body>
</html>

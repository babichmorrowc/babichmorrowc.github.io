<!DOCTYPE html>
<html lang="en" dir="ltr"><head>
  
                           
     


<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.128.0">
<title>Least squares regression: Part 1 | Cecina Babich Morrow</title>


<meta property="twitter:site" content="@babichmorrowc">
<meta property="twitter:creator" content="@babichmorrowc">







  
    
  
<meta name="description" content="Introduction to least squares regression.">


<meta property="og:site_name" content="Cecina Babich Morrow">
<meta property="og:title" content="Least squares regression: Part 1 | Cecina Babich Morrow">
<meta property="og:description" content="Introduction to least squares regression." />
<meta property="og:type" content="page" />
<meta property="og:url" content="https://babichmorrowc.github.io/blog/2024-07-02-leastsquares-regression/" />
<meta property="og:locale" content="en">




    
        <meta property="og:image" content="https://babichmorrowc.github.io/blog/2024-07-02-leastsquares-regression/featured.jpeg" >
        <meta property="twitter:card" content="summary_large_image">
        <meta name="twitter:image" content="https://babichmorrowc.github.io/blog/2024-07-02-leastsquares-regression/featured.jpeg" >
    
    
  
  <meta itemprop="name" content="Least squares regression: Part 1">
  <meta itemprop="description" content="Introduction to least squares regression.">
  <meta itemprop="datePublished" content="2024-07-02T00:00:00+00:00">
  <meta itemprop="dateModified" content="2024-07-02T00:00:00+00:00">
  <meta itemprop="wordCount" content="1311">
  <meta itemprop="image" content="https://babichmorrowc.github.io/blog/2024-07-02-leastsquares-regression/featured.jpeg">
  <meta itemprop="keywords" content="Statistics,R">
  
  
    
      <script async src="https://www.googletagmanager.com/gtag/js?id=130807002"></script>
      <script>
        var doNotTrack = false;
        if ( true ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', '130807002');
        }
      </script>
    
  


  <!--[if IE]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  <link rel="shortcut icon" href="/img/favicon_io/favicon.ico" type="image/x-icon">
  <link rel="icon" href="/img/favicon_io/favicon.ico" type="image/x-icon">
  
  
  <link rel="stylesheet" href="/style.main.min.efe606a4a38248e49cb083e10d55824649e30549b2438e91c5513e6ccbbb543d.css" integrity="sha256-7&#43;YGpKOCSOScsIPhDVWCRknjBUmyQ46RxVE&#43;bMu7VD0=" media="screen">
  
  
  <script src="/panelset.min.ed1ac24b6e16f4e2481e3d1d098ae66f5bc77438aef619e6e266d8ac5b00dc72.js" type="text/javascript"></script>
  
  
  <script src="/main.min.8775cb64fbeada525af486d0e6a80a9042ba0c7a7958fc22c03e42d5abc3fe1f.js" type="text/javascript"></script>
</head>
<body>
      <div class="grid-container">
<header class="site-header pt4 pb2 mb4 bb b--transparent ph5 headroom z-max" role="banner">
  <nav class="site-nav db dt-l w-100" role="navigation">
    <a class="site-brand db dtc-l v-mid link no-underline w-100 w-33-l tc tl-l" href="https://babichmorrowc.github.io/" title="Home">
      <img src="/img/favicon_io/favicon.ico" class="dib db-l h2 w-auto" alt="Cecina Babich Morrow">
    </a>
    <div class="site-links db dtc-l v-mid w-100 w-47-l tc tr-l mt3 mt0-l ttu tracked">
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/about/" title="About Cecina">About</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 active" href="/blog/" title="Blog">Blog</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/project/" title="Project Portfolio">Projects</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/talk/" title="Talks">Talks</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/publication/" title="Publications">Publications</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/contact/" title="Contact">Contact</a>
      
      
    </div>
  </nav>
</header>

<main class="page-main pa4" role="main">
  <section class="page-content mw7 center">
    <article class="post-content pa0 ph4-l">
      <header class="post-header">
        <h1 class="f1 lh-solid measure-narrow mb3 fw4">Least squares regression: Part 1</h1>
        <h4 class="f4 mt0 mb4 lh-title measure">Introduction to least squares regression.</h4>
        <p class="f6 measure lh-copy mv1">By Cecina Babich Morrow in <a href="https://babichmorrowc.github.io/categories/statistics">statistics</a>  <a href="https://babichmorrowc.github.io/categories/r">R</a> </p>
        <p class="f7 db mv0 ttu">July 2, 2024</p>

      

      </header>
      <section class="post-body pt5 pb4">
        


<div id="inspiration-for-this-post" class="section level2">
<h2>Inspiration for this post</h2>
<p>As evidenced by the depressing three year lag-time since my last blog post on this website, it is well past time to get back into gear and get posting once more. I am coming to the end of the first year of my PhD in Computational Statistics and Data Science, and I thought I would share some of what I learned in my courses here. Huge thanks to Dr. Song Liu for pretty much all of the material presented here (mistakes are my own).</p>
<p>I’m starting out with that beloved classic, least squares regression.</p>
</div>
<div id="making-good-choices" class="section level2">
<h2>Making good choices</h2>
<p>We’ll start out by thinking a little bit about what we need in order to make “good” decisions.</p>
<figure>
<img class="special-img-class" style="width:100%" src="/img/make_good_choices.jpeg" />
<figcaption>
How can we make Jamie Lee Curtis proud, statistically speaking?
</figcaption>
</figure>
<p>In order to make decisions rationally, a decision-making framework must fulfill the following criteria: predictions should be <strong>precise</strong> and <strong>data-driven</strong>, while accounting for both the <strong>cost</strong> of making an error in the specific situation and the <strong>randomness</strong> of the underlying data. Statistical decision-making (theoretically should) fulfill these criteria and thus provides a useful and rational framework for guiding predictions about the world.</p>
<p>Today, we’ll focus on how we might make good choices in a regression setting, i.e. predicting a continuous result based on a set of inputs. We have the following setting, in mathy terms: For an output variable <span class="math inline">\(y \in \mathbb{R}\)</span> and a set of <span class="math inline">\(d\)</span> input variables <span class="math inline">\(\textbf{x}\in \mathbb{R}^d\)</span>, we have a dataset <span class="math inline">\(D\)</span> consisting of <span class="math inline">\(n\)</span> pairs of observed inputs and outputs <span class="math inline">\(\{(\textbf{x}_i, y_i\}^n_{i=1}\)</span>.</p>
</div>
<div id="least-squares-regression" class="section level2">
<h2>Least squares regression</h2>
<p>Least squares regression tackles this regression problem by finding a prediction function <span class="math inline">\(f(\textbf{x})\)</span> that minimizes the sum of squared differences (hence the name) between the observed outputs <span class="math inline">\(y_i\)</span> and our predictions: <span class="math inline">\(\sum^n_{i=1}(y_i-f(\textbf{x}_i;\textbf{w}))^2\)</span>.</p>
<blockquote>
<p><em>Why squares?</em> By trying to minimize the sum of squared differences between prediction and reality, rather than doing something like minimizing the raw distance between prediction and reality (see <a href="https://en.wikipedia.org/wiki/Least_absolute_deviations">least absolute deviations</a>), we are penalizing the times are predictions are very far from reality even more heavily.</p>
</blockquote>
<p>Our prediction function is defined by <span class="math inline">\(f(\textbf{x};\textbf{w}) = \langle\textbf{w}_1,\textbf{x}\rangle+w_0\)</span>, where <span class="math inline">\(\textbf{w}:= [\textbf{w}_1, w_0]^\top\)</span>, and we choose parameters <span class="math inline">\(\textbf{w}\)</span> such that <span class="math inline">\(\textbf{w}_{LS} := \text{argmin}_\textbf{w} \sum_{i=1}^{n}[y_i-f(\textbf{x}_i;\textbf{w})]^2\)</span>. Solving this equation yields <span class="math inline">\(\textbf{w}_{LS}=(\textbf{X}\textbf{X}^\top)^{-1}\textbf{X}\textbf{y}^\top\)</span>, where <span class="math inline">\(\textbf{X}:=\begin{bmatrix} \textbf{x}_1,&amp;...,&amp;\textbf{x}_n\\1,&amp;...,&amp;1\end{bmatrix}\)</span>. Why? Well, let’s prove it.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-1" class="theorem"><strong>Theorem 1  (Least squares) </strong></span><span class="math inline">\(\textbf{w}_{LS}=(\textbf{X}\textbf{X}^\top)^{-1}\textbf{X}\textbf{y}^\top\)</span></p>
</div>
<div class="{proof}">
<p>Let <span class="math inline">\(\textbf{X}:=\begin{bmatrix} \textbf{x}_1,&amp;...,&amp;\textbf{x}_n\\1,&amp;...,&amp;1\end{bmatrix}\)</span>, <span class="math inline">\(\textbf{y}:=[y_1,...,y_n]\)</span>. We have defined <span class="math inline">\(\textbf{w}_{LS}\)</span> as follows: <span class="math inline">\(\textbf{w}_{LS} := \text{argmin}_\textbf{w} \sum_{i=1}^{n}[y_i-f(\textbf{x}_i;\textbf{w})]^2\)</span>.</p>
<p><span class="math display">\[\begin{align*}
    \textbf{w}_{LS} &amp;:= \text{argmin}_\textbf{w} \sum_{i=1}^{n}[y_i-f(\textbf{x}_i;\textbf{w})]^2 \\
    &amp;= \text{argmin}_\textbf{w} ||\textbf{y}-\textbf{w}^\top\textbf{X}||^2 \\
    &amp;= \text{argmin}_\textbf{w} (\textbf{y}-\textbf{w}^\top\textbf{X})(\textbf{y}-\textbf{w}^\top\textbf{X})^\top \\
    &amp;= \text{argmin}_\textbf{w} (\textbf{y}-\textbf{w}^\top\textbf{X})(\textbf{y}^\top-(\textbf{w}^\top\textbf{X})^\top) \\
    &amp;= \text{argmin}_\textbf{w} (\textbf{y}-\textbf{w}^\top\textbf{X})(\textbf{y}^\top-\textbf{X}^\top\textbf{w}) \\
    &amp;= \text{argmin}_\textbf{w} (\textbf{y}\textbf{y}^\top - 2\textbf{w}^\top\textbf{X}\textbf{y}^\top+\textbf{w}^\top\textbf{X}\textbf{X}^\top\textbf{w})
\end{align*}\]</span></p>
<p>We differentiate with respect to <span class="math inline">\(\textbf{w}\)</span> and set the result equal to zero:</p>
<p><span class="math display">\[\begin{align*}
    0 &amp;= \frac{\partial}{\partial \textbf{w}}(\textbf{y}\textbf{y}^\top - 2\textbf{w}^\top\textbf{X}\textbf{y}^\top+\textbf{w}^\top\textbf{X}\textbf{X}^\top\textbf{w}) \\
    0 &amp;= -2\textbf{X}\textbf{y}^\top+2\textbf{X}\textbf{X}^\top\textbf{w}_{LS} \\
    2\textbf{X}\textbf{y}^\top &amp;= 2\textbf{X}\textbf{X}^\top\textbf{w}_{LS}
\end{align*}\]</span></p>
<p>So <span class="math inline">\(\textbf{w}_{LS}=(\textbf{X}\textbf{X}^\top)^{-1}\textbf{X}\textbf{y}^\top\)</span>.</p>
<p>■</p>
</div>
<p>In this result, we can see that we can only solve <span class="math inline">\(\textbf{w}_{LS}\)</span> if <span class="math inline">\(\textbf{X}\textbf{X}^\top\)</span> is invertible: in situations when the sample size <span class="math inline">\(n\)</span> is less than the number of inputs <span class="math inline">\(d\)</span>, <span class="math inline">\(\textbf{X}\textbf{X}^\top\)</span> is rank deficient and thus non-invertible and the least squares solution cannot be calculated:</p>
<p>If <span class="math inline">\(n &lt; d\)</span>:</p>
<p><span class="math display">\[\begin{align*}
    \text{rank}(\textbf{XX}^\top) &amp;= \text{rank}(\textbf{X}) \\
    &amp;\leq \text{min}(d+1, n) \\
    &amp;= n
\end{align*}\]</span></p>
<p>The matrix <span class="math inline">\(\textbf{X}\textbf{X}^\top \in \mathbb{R}^{d+1 \times d+1}\)</span> and <span class="math inline">\(n &lt; d+1\)</span>, so <span class="math inline">\(\textbf{X}\textbf{X}^\top\)</span> is rank deficient and thus non-invertible.</p>
<div id="r-implementation" class="section level3">
<h3>R implementation</h3>
<p>While R obviously has built-in ways to perform least-squares regression, let’s write our own anyways:</p>
<pre class="r"><code># Function to calculate the coefficients w_LS
least_squares_solver &lt;- function(input_variables, output_variable, lambda = 0, b = 1) {
  # Set up variables
  y &lt;- output_variable
  X &lt;- t(as.matrix(input_variables)) # convert each x_i into column vector
  # Add intercept
  X &lt;- rbind(X, rep(1, ncol(X)))
  
  # Compute w_LS
  # Using what we just proved above
  w_LS &lt;- solve(X %*% t(X)) %*% X %*% as.matrix(y)
  return(w_LS)
}</code></pre>
<p>We’ll also want a function to use the <span class="math inline">\(\textbf{w}_{LS}\)</span> we find to make predictions on a new dataset:</p>
<pre class="r"><code># Function to apply coefficients to a new set of data
least_squares_predict &lt;- function(input_variables, coefficients) {
  # Set up variables
  X &lt;- t(as.matrix(input_variables)) # convert each x_i into column vector
  # Add intercept
  X &lt;- rbind(X, rep(1, ncol(X)))
  
  preds &lt;- t(as.matrix(coefficients)) %*% X
  return(preds)
}</code></pre>
<p>Finally, we’ll write a function to calculate the sum of squared errors to check how well our predictions are doing:</p>
<pre class="r"><code># Function to calculate sum of squared errors
sum_squared_errors &lt;- function(prediction, actual) {
  squared_error &lt;- (actual - prediction)^2
  return(sum(squared_error))
}</code></pre>
<p>Let’s test it out on some data. We will use a prostate cancer dataset:</p>
<pre class="r"><code>library(readr)

# Import data
prostate_data &lt;- read_csv(&quot;prostate_data.csv&quot;)</code></pre>
<pre><code>## Rows: 97 Columns: 10
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &quot;,&quot;
## dbl (9): lcavol, lweight, age, lbph, svi, lcp, gleason, pgg45, lpsa
## lgl (1): train
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<pre class="r"><code># Split data into training and test data
train_data &lt;- prostate_data[prostate_data$train == TRUE,]
test_data &lt;- prostate_data[!prostate_data$train,]</code></pre>
<p>Now, let’s use our function to make a model predicting <code>lpsa</code> using all features:</p>
<pre class="r"><code># Model using all features -----------------------------------------------------
# Fit linear least squares on the training data
train_least_squares &lt;- least_squares_solver(train_data[,1:8], train_data$lpsa)

# Apply the model to the testing data
preds &lt;- least_squares_predict(test_data[,1:8], train_least_squares)
# Calculate cross-validation error
cross_val_error &lt;- sum_squared_errors(preds, test_data$lpsa)
cross_val_error</code></pre>
<pre><code>## [1] 15.63822</code></pre>
<p>Let’s compare to the results of the built-in <code>lm</code> function from R to check that we’re on the right track:</p>
<pre class="r"><code># Compare to lm results --------------------------------------------------------
lm_model &lt;- lm(lpsa ~ ., data = train_data[,1:9])
summary(lm_model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = lpsa ~ ., data = train_data[, 1:9])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.64870 -0.34147 -0.05424  0.44941  1.48675 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.429170   1.553588   0.276  0.78334    
## lcavol       0.576543   0.107438   5.366 1.47e-06 ***
## lweight      0.614020   0.223216   2.751  0.00792 ** 
## age         -0.019001   0.013612  -1.396  0.16806    
## lbph         0.144848   0.070457   2.056  0.04431 *  
## svi          0.737209   0.298555   2.469  0.01651 *  
## lcp         -0.206324   0.110516  -1.867  0.06697 .  
## gleason     -0.029503   0.201136  -0.147  0.88389    
## pgg45        0.009465   0.005447   1.738  0.08755 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.7123 on 58 degrees of freedom
## Multiple R-squared:  0.6944,	Adjusted R-squared:  0.6522 
## F-statistic: 16.47 on 8 and 58 DF,  p-value: 2.042e-12</code></pre>
<pre class="r"><code>lm_preds &lt;- predict(lm_model, test_data)
# Check if our predictions are the same
all.equal(unname(lm_preds),
          as.vector(preds))</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>We have the same predictions on the test set!</p>
</div>
</div>
<div id="accounting-for-randomness" class="section level2">
<h2>Accounting for randomness</h2>
<p>So, are we making good choices yet? Considering our criteria for rational decision-making, least squares regression is clearly precise (motivated by mathematical principles), is computed based on observed data, and accounts for the cost of error by measuring that cost in terms of squared difference between prediction and reality. So all that is missing now is to make sure we are accounting for the randomness of the underlying data.</p>
<p>To do this, we can motivate least squares regression in a probabilistic way by assuming that our output variables <span class="math inline">\(y_i\)</span> are normally distributed with mean <span class="math inline">\(f(\textbf{x}_i;w)\)</span> and variance <span class="math inline">\(\sigma^2\)</span> and the observations <span class="math inline">\((\textbf{x}_i, y_i)\)</span> are independent and identically distributed. Then <span class="math inline">\(p(y_1, ..., y_n | \textbf{x}_1, ..., \textbf{x}_n, \textbf{w}, \sigma) = \prod_{i=1}^n N_{y_i}(f(\textbf{x}_i; w), \sigma^2)\)</span>.</p>
<p>To tune the parameters <span class="math inline">\(\textbf{w}\)</span> and <span class="math inline">\(\sigma\)</span>, we can use a technique known as maximum likelihood estimation (MLE). MLE maximizes the probability density function evaluated at the dataset <span class="math inline">\(D\)</span> with respect to our unknown parameters. Intuitively, the goal of MLE is to find parameters that make it most likely that we would observe our actual data.</p>
<p>By performing MLE to determine the parameter <span class="math inline">\(\textbf{w}_{ML}\)</span>, we find that <span class="math inline">\(\textbf{w}_{ML} = \text{argmin}_\textbf{w} \sum_{i=1}^{n}[y_i-f(\textbf{x}_i;\textbf{w})]^2 = \textbf{w}_{LS}\)</span>, so our least squares parameter is also the maximum likelihood estimator. We can also perform MLE to find the uncertainty of our prediction function <span class="math inline">\(\sigma^2_{ML} = \frac{1}{n}[y - f(\textbf{x}; \textbf{w}_{ML})]^2\)</span>, accounting for randomness in our underlying data.</p>
</div>

        
        <details closed class="f6 fw7 input-reset">
  <dl class="f6 lh-copy">
    <dt class="fw7">Posted on:</dt>
    <dd class="fw5 ml0">July 2, 2024</dd>
  </dl>
  <dl class="f6 lh-copy">
    <dt class="fw7">Length:</dt>
    <dd class="fw5 ml0">7 minute read, 1311 words</dd>
  </dl>
  
  <dl class="f6 lh-copy">
    <dt class="fw7">Categories:</dt>
    <dd class="fw5 ml0"> <a href="https://babichmorrowc.github.io/categories/statistics">statistics</a>  <a href="https://babichmorrowc.github.io/categories/r">R</a> </dd>
  </dl>
  
  
  
  <dl class="f6 lh-copy">
    <dt class="fw7">Tags:</dt>
    <dd class="fw5 ml0"> <a href="https://babichmorrowc.github.io/tags/statistics">statistics</a>  <a href="https://babichmorrowc.github.io/tags/r">R</a> </dd>
  </dl>
  
  <dl class="f6 lh-copy">
    <dt class="fw7">See Also:</dt>
    
    <dd class="fw5 ml0"><a href="/publication/bda_sensitivity/">From climate risk to action: Analysing adaptation decision robustness under uncertainty</a></dd>
    
    <dd class="fw5 ml0"><a href="/blog/2024-11-29-rsa/">Regional Sensitivity Analysis</a></dd>
    
    <dd class="fw5 ml0"><a href="/blog/2024-09-23-bias-variance/">Bias-variance decomposition</a></dd>
    
  </dl>
</details>

      </section>
      <footer class="post-footer">
        <div class="post-pagination dt w-100 mt4 mb2">
  
  
    <a class="prev dtc pr2 tl v-top fw6"
    href="https://babichmorrowc.github.io/blog/2024-07-26-regularized_leastsquares/">&larr; Least squares regression: Part 2</a>
  
  
  
    <a class="next dtc pl2 tr v-top fw6"
    href="https://babichmorrowc.github.io/blog/lifehistory-paper-highlight/">Exciting publication news! &rarr;</a>
  
</div>

      </footer>
    </article>
    
      <div class="post-comments pa0 pa4-l mt4">
  
    <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "babichmorrowc-github-io" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
  
  
</div>

    
  </section>
</main>
<footer class="site-footer pv4 bt b--transparent ph5" role="contentinfo">
  <nav class="db dt-l w-100">
    <p class="site-copyright f7 db dtc-l v-mid w-100 w-33-l tc tl-l pv2 pv0-l mv0 lh-copy">
      &copy; 2025 Cecina Babich Morrow, Anywhere
      <span class="middot-divider"></span>
      Made with <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/hugo-apero/" rel="dct:source">Hugo Apéro</a></span>.
      <br />
      
Based on <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/formspree/blogophonic-hugo" rel="dct:source">Blogophonic</a></span> by <a xmlns:cc="http://creativecommons.org/ns#" href="https://formspree.io" property="cc:attributionName" rel="cc:attributionURL">Formspree</a>.
    </p>
    
    <div class="site-social-links db dtc-l v-mid w-100 w-33-l tc pv2 pv0-l mv0">
      <div class="social-icon-links" aria-hidden="true">
  
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="/contact/" title="envelope" >
      <i class="fas fa-envelope fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://github.com/babichmorrowc" title="github" target="_blank" rel="me noopener">
      <i class="fab fa-github fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://scholar.google.com/citations?user=20mEEooAAAAJ&amp;hl=en" title="google-scholar" target="_blank" rel="me noopener">
      <i class="fab fa-google-scholar fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://orcid.org/my-orcid?orcid=0000-0003-2188-1495" title="orcid" target="_blank" rel="me noopener">
      <i class="fab fa-orcid fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://x.com/babichmorrowc" title="x-twitter" target="_blank" rel="me noopener">
      <i class="fab fa-x-twitter fa-lg fa-fw"></i>
    </a>
  
</div>

    </div>
    
    <div class="site-links f6 db dtc-l v-mid w-100 w-67-l tc tr-l pv2 pv0-l mv0">
      
      <a class="dib pv1 ph2 link" href="/license/" title="License">License</a>
      
      <a class="dib pv1 ph2 link" href="/contact/" title="Contact form">Contact</a>
      
      <a class="dib pv1 ph2 link" href="/contributors/" title="Contributors">Contributors</a>
      
    </div>
  </nav>
  
    <script>

    var i, text, code, codes = document.getElementsByTagName('code');
    for (let i = 0; i < codes.length;) {
      code = codes[i];
      if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
        text = code.textContent;
        if (/^\$[^$]/.test(text) && /[^$]\$$/.test(text)) {
          text = text.replace(/^\$/, '\\(').replace(/\$$/, '\\)');
          code.textContent = text;
        }
        if (/^\\\((.|\s)+\\\)$/.test(text) ||
            /^\\\[(.|\s)+\\\]$/.test(text) ||
            /^\$(.|\s)+\$$/.test(text) ||
            /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
          code.outerHTML = code.innerHTML;  
          continue;
        }
      }
      i++;
    }
</script>

  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css" integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js" integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>



    
  
  
</footer>

      </div>
    </body>
</html>

[{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536465600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536465600,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"/tutorial/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":["R"],"content":"   In keeping with my Lego theme, this statue from Nathan Sawaya’s exhibition at the New York Hall of Science represents some of the frustrations of adding fonts to R.   Inspiration for this post I’ve been working on creating a visualization cookbook for R graphics (check out this great example from the BBC to see what I’m eventually going for). As part of this process, I wanted to be able to change the font of my plots to Source Sans Pro, which is a Google font.\n sysfonts package The first step here was to install and load the sysfonts package:\nlibrary(sysfonts) XQuartz If you end up getting the following error when loading sysfonts: Reason: image not found, you might need to do some additional work. This happened to me initially, and after doing some Googling, I found this GitHub issue. Installing XQuartz was recommended as a fix, particularly for Macs, so I downloaded it from here. After downloading XQuartz, I uninstalled, reinstalled, and loaded sysfonts and things went smoothly.\n  Downloading font to your computer As I learned from this super helpful GitHub repo, you need to install the font on your system as well as within R.\nMac I went through this process on a Mac, so this is what I ended up doing to install the font. I went to the Google Fonts page for the font and clicked on “Select this font”. Then after clicking on “1 Family Selected” at the bottom, I downloaded the font by clicking on the download icon. This downloaded a zip drive into my Downloads folder. To install it as a font, I extracted the zip folder (by opening the zip drive). In a different Finder window, I opened my Applications folder and found Font Book. Then I dragged the zip folder over into Font Book.\n PC I modified the above instructions from these instructions on Flourish Online, which also has PC instructions. If anyone gives these a try and they don’t work, let me know and I’ll try to do some troubleshooting!\n  font_add_google Next, I used the font_add_google function from sysfonts to download the Source Sans Pro fonts:\nfont_add_google(\u0026quot;Source Sans Pro\u0026quot;) If you want to install a different Google Font, you can run font_families_google() to see the list of family names of fonts currently available in Google Fonts:\nhead(font_families_google()) ## [1] \u0026quot;ABeeZee\u0026quot; \u0026quot;Abel\u0026quot; \u0026quot;Abhaya Libre\u0026quot; \u0026quot;Abril Fatface\u0026quot; ## [5] \u0026quot;Aclonica\u0026quot; \u0026quot;Acme\u0026quot;  Example To continue with my Lego theme, I’ll demonstrate using this font on data from the legocolors package. We can make the following graph of approximate lego brick availability based on the year a brick was released (colored by the brick color!):\nlibrary(legocolors) ## Warning: package \u0026#39;legocolors\u0026#39; was built under R version 3.5.2 library(ggplot2) ## Warning: package \u0026#39;ggplot2\u0026#39; was built under R version 3.5.2 ggplot(legocolors, aes(x = year_released, y = bl_bp, colour = hex)) + geom_point() + scale_color_manual(values = legocolors$hex) + theme_minimal() + theme(legend.position = \u0026quot;none\u0026quot;, text = element_text(family = \u0026quot;Source Sans Pro\u0026quot;), plot.title = element_text(face = \u0026quot;bold\u0026quot;)) + labs(x = \u0026quot;Year released\u0026quot;, y = \u0026quot;Brick availability\u0026quot;, title = \u0026quot;Lego availability\u0026quot;, subtitle = \u0026quot;All of the text is now in Source Sans Pro!\u0026quot;) ## Warning: Removed 3 rows containing missing values (geom_point).  Troubleshooting This process is a little bit finicky, so there are certain errors that crop up frequently. For me, just restarting RStudio and/or my computer (or updating the computer, if necessary) has solved these. The most common errors are No font could be found for family \u0026quot;Source Sans Pro\u0026quot; and \u0026quot;Error in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : polygon edge not found\u0026quot;, both of which relate to R not being able to find the font. If the font is indeed installed on your computer (you can check this in Font Book, for a Mac), then restarting things should help R find the font.\n ","date":1570846394,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570846394,"objectID":"76bb561003b5ecca107970e27a1c827f","permalink":"/post/2019-10-11-google_fonts/","publishdate":"2019-10-11T21:13:14-05:00","relpermalink":"/post/2019-10-11-google_fonts/","section":"post","summary":"In keeping with my Lego theme, this statue from Nathan Sawaya’s exhibition at the New York Hall of Science represents some of the frustrations of adding fonts to R.   Inspiration for this post I’ve been working on creating a visualization cookbook for R graphics (check out this great example from the BBC to see what I’m eventually going for). As part of this process, I wanted to be able to change the font of my plots to Source Sans Pro, which is a Google font.","tags":["R"],"title":"Putting Google Fonts in R graphs","type":"post"},{"authors":null,"categories":["R"],"content":"   A lego fort that I constructed this summer with the help of a good friend (not particularly relevant to the post, except that I used a lego dataset for my example and legos are more photogenic than Mode databases).   Inspiration for this post In the past, I’ve been used to a pretty straightforward R workflow: download my data as a csv from somewhere, save it on my computer, and go to work. Now that I work in healthcare, downloading data and saving it on my computer is no longer such a good idea (since that would be illegal) so I’ve needed other ways to get the data I need into R. My company uses the data analysis platform Mode Analytics to pull data from our database using SQL. I wanted a way to get data directly from Mode into R without needing to download the results of a report as a CSV, move them onto Google Drive, and then access them in R.\n Get API access token The first step to accessing Mode via API is to generate a token. First, log in to Mode, click on your name in the top left corner, and select “My Account”. Then click on “API Tokens” on the left side. You can generate a token by specifying a name for the token and clicking “Create token”. The resulting credentials consist of your token (a.k.a. username or access key) and your password (a.k.a. access secret). This token is associated with your Mode user account, not an organization, so you’ll be able to access reports from any organization you have access to.\n Export report CSV to dataframe The following instructions essentially follow the Python code instructions on the Mode website to export a report to a csv. This code doesn’t actually download the csv file, however, just imports it into R as a dataframe (so no datafiles on your computer!).\nThe following functions rely on the httr and jsonlite packages:\nlibrary(httr) library(jsonlite) You also need your username and password, preferably assigned to variables in R. For this example, my username is assigned to username and my password is assigned to password.\n# Replace with your username and password username \u0026lt;- \u0026quot;YOUR_USERNAME\u0026quot; password \u0026lt;- \u0026quot;YOUR_PASSWORD\u0026quot; You also need to find the token of the report you’re interested in. When you go to your report in Mode, the token is everything after “reports/”. So, for example, if you’re interested in Legos (which you should be), you can get the most recent run of the Lego set summary report. The URL is https://app.mode.com/modeanalytics/reports/eb4fdefe37d8, so the report token is “eb4fdefe37d8”.\n# Replace with the report token of interest report_token \u0026lt;- \u0026quot;eb4fdefe37d8\u0026quot; Finally, you need the name of your organization on Mode. For this example, we’ll be looking at a community available Mode report, so the organization is “modeanalytics”. You can get this from the URL as well, it will be the part directly before “/reports”:\n# Replace with the organization of interest organization \u0026lt;- \u0026quot;modeanalytics\u0026quot; Get the latest run token The first step is to find the run token of the most recent time the Mode report was run. The following function gets that token, or throws an error if the latest run of the report was not successful:\nget_report_latest_run \u0026lt;- function(report_token, organization, username, password){ # Generate the URL for runs of the report you\u0026#39;re interested in url = paste0(\u0026quot;https://modeanalytics.com/api/\u0026quot;, organization, \u0026quot;/reports/\u0026quot;, report_token, \u0026quot;/runs\u0026quot;) r = GET(url, authenticate(username, password, type = \u0026quot;basic\u0026quot;)) result = fromJSON(content(r, \u0026quot;text\u0026quot;)) # Find the most recent run token most_recent_report_run_token \u0026lt;- result$`_embedded`$report_runs[1,]$token # Find the status of the most recent run status = result$`_embedded`$report_runs[1,]$state if(status == \u0026quot;succeeded\u0026quot;){ # if the most recent report ran, return(most_recent_report_run_token) # return that run token } else{ stop(\u0026quot;Report latest run did not succeed.\u0026quot;) # if the most recent run failed, error } } So, for our example report, the most recent run token was:\nget_report_latest_run(report_token, organization, username, password) ## No encoding supplied: defaulting to UTF-8. ## [1] \u0026quot;079bc1243d11\u0026quot;  Extract the data from the most recent run Now, we can use a second function (which calls get_report_latest_run) to get the data from the most recent run:\nexport_report_to_csv \u0026lt;- function(report_token, organization, username, password) { # Get the latest run token latest_report_run_token \u0026lt;- get_report_latest_run(report_token, organization, username, password) # Generate the URL for the latest run of the report as a csv url \u0026lt;- paste0(\u0026quot;https://modeanalytics.com/api/\u0026quot;, organization, \u0026quot;/reports/\u0026quot;, report_token, \u0026quot;/runs/\u0026quot;, latest_report_run_token, \u0026quot;/results/content.csv\u0026quot;) r \u0026lt;- GET(url, authenticate(username, password, type = \u0026quot;basic\u0026quot;)) # Read the data as a csv r_csv \u0026lt;- content(r, type = \u0026quot;text/csv\u0026quot;) return(r_csv) }   Example To get the data from the most recent run of the Lego set summary report, you want to save a script file containing the above two functions (I call this script mode_api_script.R. Then, run the following:\nlibrary(httr) library(jsonlite) source(\u0026quot;mode_api_script.R\u0026quot;) # Set username, password, and report token username \u0026lt;- \u0026quot;YOUR_USERNAME\u0026quot; password \u0026lt;- \u0026quot;YOUR_PASSWORD\u0026quot; report_token \u0026lt;- \u0026quot;eb4fdefe37d8\u0026quot; lego_data \u0026lt;- export_report_to_csv(report_token, organization, username, password) ## No encoding supplied: defaulting to UTF-8. ## No encoding supplied: defaulting to UTF-8. ## Parsed with column specification: ## cols( ## year = col_integer(), ## white = col_double(), ## red = col_double(), ## blue = col_double(), ## black = col_double(), ## light_gray = col_double(), ## total = col_double(), ## main = col_character() ## ) knitr::kable(head(lego_data))   year white red blue black light_gray total main    1950 6 0 0 0 0 6 White was the main color  1953 12 0 0 0 0 12 White was the main color  1954 6 0 0 0 0 6 White was the main color  1955 21 0 0 0 0 21 White was the main color  1956 9 0 0 0 0 9 White was the main color  1957 45 0 0 0 0 45 White was the main color     ","date":1569550394,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569550394,"objectID":"8728aa003108767323a272a8506117d8","permalink":"/post/2019-09-26-mode_api_instructions/","publishdate":"2019-09-26T21:13:14-05:00","relpermalink":"/post/2019-09-26-mode_api_instructions/","section":"post","summary":"A lego fort that I constructed this summer with the help of a good friend (not particularly relevant to the post, except that I used a lego dataset for my example and legos are more photogenic than Mode databases).   Inspiration for this post In the past, I’ve been used to a pretty straightforward R workflow: download my data as a csv from somewhere, save it on my computer, and go to work.","tags":["R"],"title":"Accessing Mode data in R","type":"post"},{"authors":null,"categories":["GitHub"],"content":"   From xkcd.   Inspiration for this post Over the coming months, I plan on sharing a series of Git workflow tutorials. Getting a Git project set up can be a pretty simple process, but depending on the order in which you do things, set-up can get complicated. This first tutorial addresses one of those fairly complicated situations.\nThe schema for these tutorials is loosely based on the incredible Git + R bible by Jenny Bryan, Happy Git and GitHub for the useR. The only difference is that I will be showing the workflow for setting up the project using the command line, rather than RStudio.\n Existing project, GitHub last This first tutorial addresses the following situation: you have a folder for your project on your computer already. You would like this folder to be associated with a GitHub repository, but you haven’t created this repository yet, or you’ve created the GitHub repository and haven’t connected it with your local folder [^1].\n[^1] Throughout this tutorial, “local” refers to the copy of the repository on your computer. “Remote” refers to the copy on GitHub.\nMake a Git repo The first step in this process is to make your local project into a Git repository. Start by using the command-line to navigate to the folder for your project (using cd). Once you are in the proper folder, run:\ngit init This command either creates a new Git repository, or, in our case, converts an existing project into a Git repository. If you want to read more about git init, particularly the difference between git init and git clone, Atlassian has a useful article.\nNow, since you’ve made edits / added files into your project, you want to add and commit those changes using the following commands:\ngit add -A git commit -m \u0026quot;test commit\u0026quot; You can make the commit message whatever you want. Note, git add -A adds all files in your repository. If you only want to add certain files, you can add them by name. For example, if you only want to add the file test.txt:\ngit add test.txt git commit -m \u0026quot;test commit\u0026quot;  Make a GitHub repo Next, we need to create a GitHub repository for our project. Depending on your workflow, you may have already created this repository. If not, log in to GitHub. Click the green “New repository” button. Ideally, you want the name of your new repository to match the name of the folder on your computer (but if not, it’s not a big deal). You don’t really want to initialize the repository with a README, but if you do, you can work around this later.\nOnce the repository has been set up, you can copy the HTTPS URL to your clipboard using the green “Clone or Download” button.\n Connect the GitHub repo to your project Now that you have a GitHub repository and you have a Git repository on your computer, you want to connect the two. To do this, run the following in the shell, using the URL that you just copied from GitHub:\ngit remote add origin \u0026lt;YOUR URL HERE\u0026gt; Note, you don’t need the \u0026lt; and \u0026gt; on either side of the URL, just paste the URL as is.\nYou now need to tell your computer what the upstream branch should be, i.e. what branch on GitHub to pull updates from.\ngit branch --set-upstream-to=origin/master master  Reconcile the remote and local repositories Now, assuming that you have edits both on your computer and on the GitHub repository, you need to be able to reconcile these changes so that the two are the same. This is assuming that you have files added in the folder on your computer and also something like a README or .gitignore on GitHub. To fix this problem, you first need to pull the changes from the GitHub repository onto your computer:\ngit pull --allow-unrelated-histories The --allow-unrelated-histories portion of the command allows you to merge these two repositories (the local and the remote) even though they don’t have a common ancestor (you created them independently and made changes to both independently).\nNow, you can push the changes from your local repository up to GitHub:\ngit push   ","date":1568772794,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568772794,"objectID":"acb3c6aa54a28f52871830df398388e3","permalink":"/post/2019-09-18-git-workflow1/","publishdate":"2019-09-17T21:13:14-05:00","relpermalink":"/post/2019-09-18-git-workflow1/","section":"post","summary":"From xkcd.   Inspiration for this post Over the coming months, I plan on sharing a series of Git workflow tutorials. Getting a Git project set up can be a pretty simple process, but depending on the order in which you do things, set-up can get complicated. This first tutorial addresses one of those fairly complicated situations.\nThe schema for these tutorials is loosely based on the incredible Git + R bible by Jenny Bryan, Happy Git and GitHub for the useR.","tags":["GitHub"],"title":"Git Workflow #1: Existing project, GitHub last","type":"post"},{"authors":null,"categories":["Career"],"content":"  This is just a quick post to provide a link to the Goldwater Scholar Social Media Highlight that Nina Singh wrote about me. You can find the pdf to the article here. Huge thanks to the Goldwater social media team for including me in this!\n","date":1566345600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566345600,"objectID":"717f42d4aaa0e505e9c3f62188b6e120","permalink":"/post/goldwater-highlight/","publishdate":"2019-08-21T00:00:00Z","relpermalink":"/post/goldwater-highlight/","section":"post","summary":"This is just a quick post to provide a link to the Goldwater Scholar Social Media Highlight that Nina Singh wrote about me. You can find the pdf to the article here. Huge thanks to the Goldwater social media team for including me in this!","tags":["career"],"title":"Goldwater Highlight","type":"post"},{"authors":["Cecina Babich Morrow","Jamie M. Kass","Peter J. Galante","Robert P. Anderson","Mary E. Blair"],"categories":null,"content":"","date":1565323200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565323200,"objectID":"09c270455b94601bd075735c3909951a","permalink":"/talk/ibs_2019/","publishdate":"2019-08-09T00:00:00-04:00","relpermalink":"/talk/ibs_2019/","section":"talk","summary":"Despite growing understanding that biotic interactions may impose important constraints on distributional limits, species distribution modeling (SDM) applications typically focus on abiotic variables without explicitly accounting for biotic interactions. One example of biotic influences on geographic ranges is the common phenomenon of closely related parapatric species replacing each other across geography. We sought to address whether incorporating biotic information via post-processing SDM outputs would improve distributional estimates for three sloth species in the genus Bradypus with parapatric ranges. In a novel approach, we used support vector machines (SVMs) to spatially classify map cells, indicating which of the species is most likely to be present. We then use the SVM output to mask the SDM suitability predictions by removing pixels that the SVM indicated as part of the range of a different species. We implemented two different types of SVMs: 1) purely spatial SVMs, using only occurrence data, and 2) spatial + environmental SVMs, using occurrence data in conjunction with SDM-predicted suitability values. After using the SVM outputs as masks, we found that both SVM implementations were less likely to include occurrence points of congeners in species’ predicted distributions than the unmodified SDM predictions. Further, we found that the spatial + environmental SVM resulted in distributional delimitations for contact zones that best matched our ecological expectations for these species. This approach could be widely applied both to pure macroecological studies as well as conservation and management, including IUCN Red Listing.","tags":["species distribution modeling","Bradypus","biogeography"],"title":"Talk: Delineating parapatric ranges using species distribution models and support vector machines: An example with three-toed sloths (Bradypus)","type":"talk"},{"authors":["Cecina Babich Morrow","Peter J. Galante","Jamie M. Kass","Mary E. Blair"],"categories":null,"content":"","date":1561780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561780800,"objectID":"1885267db2f0987834ad94abe5692b22","permalink":"/talk/mammalogy_2019/","publishdate":"2019-06-29T00:00:00-04:00","relpermalink":"/talk/mammalogy_2019/","section":"talk","summary":"Three-toed sloths (genus Bradypus) include three species distributed parapatrically across mainland Central and South America: B. variegatus, B. tridactylus, and B. torquatus. The distributions of these arboreal species are constrained by forest cover, as well as by the presence of their parapatric congeners. Traditional species distribution modeling (SDM) techniques typically rely on abiotic variables without accounting for the effects of competition on range boundaries. We sought to improve range predictions for Bradypus by using the ranges of the parapatric congeners to mask abiotic SDM range predictions. To account for the presence of parapatric species, we used support vector machines (SVMs) to delineate borders between ranges using occurrence data and predicted SDM habitat suitabilities for each species. We found that the SVM range estimates were more closely aligned with ecological expectations than those generated from unmodified species distribution model predictions. The SVM estimates were also less likely to include occurrences of parapatric species in the predicted range and better accounted for known contact zones between species. Finally, we masked the range estimates with a forest cover threshold calculated by temporally matching occurrence points with remotely sensed forest cover data. This mask quantified the differential effects of deforestation and habitat fragmentation across the three species’ ranges.","tags":["Bradypus","species distribution modeling","biogeography","machine learning"],"title":"Poster: Improving species range estimates for an arboreal species group with a parapatric distribution","type":"talk"},{"authors":["Cecina Babich Morrow"],"categories":null,"content":"\n","date":1560398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560398400,"objectID":"0bd6b3a341eb73ce5780e8e5fca0224c","permalink":"/talk/helenfellow_june2019/","publishdate":"2019-06-13T00:00:00-04:00","relpermalink":"/talk/helenfellow_june2019/","section":"talk","summary":"Species distribution modeling (SDM) techniques are a common tool for estimating species ranges. These models typically rely only only on abiotic variables without accounting for biotic interactions, despite the fact that these interactions may impose important constraints on ranges. Distribution patterns in which closely-related parapatric species replace each other across geographic space are common in ecology. We sought to address whether incorporating biotic information into range estimates for three species of sloth (genus *Bradypus*) would improve distribution models for species demonstrating this parapatric pattern of distribution. We used support vector machines (SVMs) as masks to delineate the predicted boundaries between these three species' ranges. We created two different kinds of SVMs: 1) spatial SVMs using only occurrence data, and 2) sp+env SVMs using occurrence data in conjunction with predicted habitat suitability from SDMs. We found that the sp+env SVM resulted in the most ecologically realistic distribution model, accounting for contact zones between species and the effects of climate.","tags":["species distribution modeling","Bradypus","machine learning"],"title":"Talk: Using SVMs to delineate parapatric ranges: An example with three-toed sloths (Bradypus)","type":"talk"},{"authors":null,"categories":["R"],"content":"   From Wild Green Memes for Ecological Fiends.   Inspiration for this post A few people in my lab are headed to a workshop in Colombia next week, and due to the possibility of intermittent wi-fi, they need to load all of the workshop materials onto USB drives. They need R with certain packages on the USBs in versions compatible with both Macs and PCs, so I offered to help out by loading my Mac version of R and my packages.\n Loading R onto a USB The first step was to copy my installation of R onto the USB. To do this, I just copied R from my Applications folder onto the folder. We are not entirely sure if this will work yet to transfer R to someone else’s computer (if someone knows the answer, let me know!), but we are going to test it out shortly.\n Loading R packages onto a USB The next step was to transfer all of my R packages onto the USB. To do this, I first needed to figure out where the packages were saved on my computer. You can run the following in R to find where your packages are saved:\n.libPaths() ## [1] \u0026quot;/Library/Frameworks/R.framework/Versions/3.5/Resources/library\u0026quot; Next, I needed to figure out how to navigate to the USB in Terminal. According to this StackOverflow answer, all drives are mounted in /Volumes, so I ran the following in Terminal:\ncd /Volumes ls From there, I was able to see USB DISK as one of the folders in /Volumes. In order to copy all of the packages from my computer onto the USB, I used cp to move everything from the folder found in .libPaths() onto the USB:\n# replace the first file path with what you get from .libPaths() cp -R /Library/Frameworks/R.framework/Versions/3.5/Resources/library/ /Volumes/USB\\ DISK/R_packages/  ","date":1559959994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559959994,"objectID":"d419056c49211ae5058e0f63ef5a9874","permalink":"/post/2019-06-07-usb/","publishdate":"2019-06-07T21:13:14-05:00","relpermalink":"/post/2019-06-07-usb/","section":"post","summary":"From Wild Green Memes for Ecological Fiends.   Inspiration for this post A few people in my lab are headed to a workshop in Colombia next week, and due to the possibility of intermittent wi-fi, they need to load all of the workshop materials onto USB drives. They need R with certain packages on the USBs in versions compatible with both Macs and PCs, so I offered to help out by loading my Mac version of R and my packages.","tags":["R"],"title":"Copying R libraries to a USB","type":"post"},{"authors":null,"categories":["R"],"content":"   Bizarro.   Inspiration for this post I recently needed to make some figures to present and noticed that the ones I was importing from saved R plots were showing up fuzzy on the presentation. Shortly afterwards, I had interns trying to generate figures with the same aspect ratio from different computers. I did a little digging and found this method for saving figures with specified resolutions and aspect ratios.\n Solution In order to save your figure with a certain size and resolution, you just need to include your code for plotting between the two lines below:\n# To save a .png file: png(\u0026quot;your_image.png\u0026quot;, units = \u0026quot;in\u0026quot;, width = 5, height = 4, res = 300) # your plotting code here dev.off() You can also save .bmp, .jpeg, and .tiff files in the same way with the bmp, jpeg and tiff functions.\n Example To see this in practice, we can plot some data from the iris dataset the “regular” way:\npairs(iris[,1:4], col = iris$Species) Things look pretty nice in the plotting window, but when you save that figure from RStudio, this is the result:\n  To get rid of that graininess, we can instead save the figure in the following way:\npng(\u0026quot;highres.png\u0026quot;, units = \u0026quot;in\u0026quot;, width = 5, height = 4, res = 300) pairs(iris[,1:4], col = iris$Species) dev.off() The resulting figure is much clearer and ready to be inserted in presentations or publications!\n  You can also change the aspect ratio by modifying the height and width arguments. You can leave the units in inches or set it to pixels (\u0026quot;px\u0026quot;), centimeters (\u0026quot;cm\u0026quot;), or millimeters (\u0026quot;mm\u0026quot;).\n ","date":1558663994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558663994,"objectID":"7cbd481411ef07a6b5cce3e45e7d21cd","permalink":"/post/2019-05-23-highres-figures/","publishdate":"2019-05-23T21:13:14-05:00","relpermalink":"/post/2019-05-23-highres-figures/","section":"post","summary":"Bizarro.   Inspiration for this post I recently needed to make some figures to present and noticed that the ones I was importing from saved R plots were showing up fuzzy on the presentation. Shortly afterwards, I had interns trying to generate figures with the same aspect ratio from different computers. I did a little digging and found this method for saving figures with specified resolutions and aspect ratios.","tags":["R"],"title":"Setting resolution and aspect ratios in R","type":"post"},{"authors":null,"categories":["R","species distribution modeling"],"content":"   From left to right: unmodified species distribution model, minimum training presence threshold, and 10th percentile threshold.   Inspiration for this post Conservation is often the main motivation behind studying where a species lives – having a model of a species’ range can help scientists assess whether it is at risk of extinction, designate protected regions to preserve its habitat, and study potential impacts of human activity. When we create species distribution models using common methods like Maxent, the result is a map of predicted habitat suitability or probability of species presence, such as the one below. In conservation management, however, it is often more useful to present range models in the form of species presence/absence. We can convert continuous predictions of habitat suitability into binary predictions of whether a species lives in a certain region or not using thresholds: i.e. designating all regions above a certain suitability level as within the species range and all areas below that suitability level as outside of it.\n  Left: species distribution model with continuous habitat suitability values. Right: binary presence/absence model used by applying a threshold. (Figure from Spatial Data Science with R)   I recently needed to threshold some species distribution models to convert them into these binary maps and had difficulty finding a built-in way to do this in R. The dismo package for species distribution modeling has a function threshold to find what value to use as the “cut-off”, but I needed a function to apply a given cut-off value to model and output a raster with binary values for presence and absence.\n Thresholding function I wrote an R function to take a species distribution model and threshold it by a given threshold - either minimum training presence (MTP) or 10th percentile training present (P10).\nMinimum training presence This threshold finds the lowest predicted suitability value for an occurrence point. Essentially, it assumes that the least suitable habitat at which the species is known to occur is the minimum suitability value for the species. The MTP threshold ensures that all occurrence points fall within the area of the binary model.\n 10th percentile training presence The P10, on the other hand, is a threshold which omits all regions with habitat suitability lower than the suitability values for the lowest 10% of occurrence records. It assumes that the 10% of occurrence records in the least suitable habitat aren’t occurring in regions that are representative of the species overall habitat, and thus should be omitted. This threshold omits a greater region than the MTP.\n The function The following is the function I wrote to apply these two thresholds to an SDM. The function’s arguments are the SDM, the occurrence points of the species in the form of longitude - latitude pairs, the threshold type, and whether the user would like the output to be a binary prediction (0s for predicted absence and 1s for predicted presence), or a thresholded continuous SDM (regions with suitability below the threshold set to 0).\nlibrary(raster) ## Warning: package \u0026#39;raster\u0026#39; was built under R version 3.5.2 ## Loading required package: sp sdm_threshold \u0026lt;- function(sdm, occs, type = \u0026quot;mtp\u0026quot;, binary = FALSE){ occPredVals \u0026lt;- raster::extract(sdm, occs) if(type == \u0026quot;mtp\u0026quot;){ thresh \u0026lt;- min(na.omit(occPredVals)) } else if(type == \u0026quot;p10\u0026quot;){ if(length(occPredVals) \u0026lt; 10){ p10 \u0026lt;- floor(length(occPredVals) * 0.9) } else { p10 \u0026lt;- ceiling(length(occPredVals) * 0.9) } thresh \u0026lt;- rev(sort(occPredVals))[p10] } sdm_thresh \u0026lt;- sdm sdm_thresh[sdm_thresh \u0026lt; thresh] \u0026lt;- NA if(binary){ sdm_thresh[sdm_thresh \u0026gt;= thresh] \u0026lt;- 1 } return(sdm_thresh) } The first step of the function is to extract the SDM predictions at all occurrence points.\noccPredVals \u0026lt;- raster::extract(sdm, occs) Next, the function calculates a threshold value thresh for either the MTP or P10 threshold. Finally, it sets all cells in the SDM raster with values lower than the threshold equal to 0. If the user wants a binary map, the function sets all cells above the threshold equal to 1:\nsdm_thresh \u0026lt;- sdm sdm_thresh[sdm_thresh \u0026lt; thresh] \u0026lt;- NA if(binary){ sdm_thresh[sdm_thresh \u0026gt;= thresh] \u0026lt;- 1 }   Example Now we can apply the function to an actual SDM I generated for a species of three-toed sloth (Bradypus variegatus).\n# load in the SDM and occurrence points sloth_sdm \u0026lt;- raster(\u0026quot;/Users/hellenfellows/Desktop/website-hugo/static/SDMs/variegatus_sdm.tif\u0026quot;) sloth_occs \u0026lt;- read.csv(\u0026quot;~/Desktop/website-hugo/static/SDMs/variegatus_occ.csv\u0026quot;) plot(sloth_sdm) points(sloth_occs[,2:3], pch = 19, cex = 0.5) We can apply both MTP and P10 thresholds to the SDM based on the location of the occurrence points:\nsloth_mtp \u0026lt;- sdm_threshold(sloth_sdm, sloth_occs[,2:3], \u0026quot;mtp\u0026quot;) plot(sloth_mtp)  sloth_p10 \u0026lt;- sdm_threshold(sloth_sdm, sloth_occs[,2:3], \u0026quot;p10\u0026quot;) plot(sloth_p10) We could also make either of these thresholded SDMs into a binary prediction in the following way:\nsloth_mtp_bin \u0026lt;- sdm_threshold(sloth_sdm, sloth_occs[,2:3], \u0026quot;mtp\u0026quot;, binary = TRUE) plot(sloth_mtp_bin)  Generalization My primary motivation to write this function was to use it on SDMs, but the function could easily be generalized to threshold any raster by a given value:\nraster_threshold \u0026lt;- function(input_raster, points = NULL, type = NULL, threshold = NULL, binary = FALSE) { if (!is.null(points)) { pointVals \u0026lt;- raster::extract(input_raster, points) if (type == \u0026quot;mtp\u0026quot;) { threshold \u0026lt;- min(na.omit(pointVals)) } else if (type == \u0026quot;p10\u0026quot;) { if (length(pointVals) \u0026lt; 10) { p10 \u0026lt;- floor(length(pointVals) * 0.9) } else { p10 \u0026lt;- ceiling(length(pointVals) * 0.9) } threshold \u0026lt;- rev(sort(pointVals))[p10] } } raster_thresh \u0026lt;- input_raster raster_thresh[raster_thresh \u0026lt; threshold] \u0026lt;- NA if (binary) { raster_thresh[raster_thresh \u0026gt;= threshold] \u0026lt;- 1 } return(raster_thresh) } I expanded the function to allow the user to input points within the raster to calculate MTP and P10 thresholds if desired, but also to enable a user-specified threshold.\n# create arbitrary raster raster1 \u0026lt;- raster(nrow=10, ncol=10) raster1[1:25]\u0026lt;- 1:25 raster1[26:50] \u0026lt;- rev(1:25) raster1[51:75] \u0026lt;- 1:25 raster1[76:100] \u0026lt;- rev(1:25) # create a set of 20 arbitrary points within the raster xy \u0026lt;- data.frame(x = runif(20, min = -150, max = 150), y = runif(20, min = -70, max = 70)) plot(raster1) points(xy) Now we can apply the function to see the MTP and P10 thresholded rasters:\nmtp_raster \u0026lt;- raster_threshold(input_raster = raster1, points = xy, type = \u0026quot;mtp\u0026quot;, binary = TRUE) plot(mtp_raster)  p10_raster \u0026lt;- raster_threshold(input_raster = raster1, points = xy, type = \u0026quot;p10\u0026quot;, binary = TRUE) plot(p10_raster) We can also use a user-inputted threshold to remove all parts of the raster with values lower than 20:\nuser_raster \u0026lt;- raster_threshold(input_raster = raster1, threshold = 20) plot(user_raster)  ","date":1555121594,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555121594,"objectID":"a4e7764e373155e2f880c771fe33176f","permalink":"/post/2019-04-12-sdm-threshold/","publishdate":"2019-04-12T21:13:14-05:00","relpermalink":"/post/2019-04-12-sdm-threshold/","section":"post","summary":"From left to right: unmodified species distribution model, minimum training presence threshold, and 10th percentile threshold.   Inspiration for this post Conservation is often the main motivation behind studying where a species lives – having a model of a species’ range can help scientists assess whether it is at risk of extinction, designate protected regions to preserve its habitat, and study potential impacts of human activity. When we create species distribution models using common methods like Maxent, the result is a map of predicted habitat suitability or probability of species presence, such as the one below.","tags":["R","species distribution modeling"],"title":"Thresholding species distribution models","type":"post"},{"authors":["Cecina Babich Morrow"],"categories":null,"content":"\n","date":1554264000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554264000,"objectID":"1243fac46afe443a0ead6b4700e001a0","permalink":"/talk/brownscholars_ratiotalk_2019/","publishdate":"2019-04-03T00:00:00-04:00","relpermalink":"/talk/brownscholars_ratiotalk_2019/","section":"talk","summary":"Organisms on Earth exhibit a staggering diversity of sizes: tetrapods range in mass from some of the tiniest frogs all the way to the blue whale. How can we compare these organisms in a meaningful way that goes beyond their differences in size? To study diversity in life history strategies across organisms with a range of body masses, my undergraduate research used dimensionless measures, or ratios. These ratios allowed me to look at differences in the ways amphibians, reptiles, birds, and mammals allocate resources to survival and reproduction. This strategy of using ratios to compare animals of vastly different sizes is also useful in neurobiology. By calculating brain to body mass ratios, rather than focusing on brain mass alone, we can study how different organisms allocate resources to their brains.","tags":["life history","education"],"title":"Big vs. Small: Using ratios to compare life history strategies","type":"talk"},{"authors":["Cecina Babich Morrow"],"categories":null,"content":"\n","date":1552622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552622400,"objectID":"fc3546731550ee72002dffd804bf084e","permalink":"/talk/nysdm_march2019/","publishdate":"2019-03-15T00:00:00-04:00","relpermalink":"/talk/nysdm_march2019/","section":"talk","summary":"Species distribution modeling (SDM) techniques are a common tool for estimating species ranges. These models typically rely only only on abiotic variables without accounting for biotic interactions, despite the fact that these interactions may impose important constraints on ranges. Distribution patterns in which closely-related parapatric species replace each other across geographic space are common in ecology. We sought to address whether incorporating biotic information into range estimates for three species of sloth (genus *Bradypus*) would improve distribution models for species demonstrating this parapatric pattern of distribution. We used support vector machines (SVMs) as masks to delineate the predicted boundaries between these three species' ranges. We created two different kinds of SVMs: 1) spatial SVMs using only occurrence data, and 2) sp+env SVMs using occurrence data in conjunction with predicted habitat suitability from SDMs. We found that the sp+env SVM resulted in the most ecologically realistic distribution model, accounting for contact zones between species and the effects of climate.","tags":["species distribution modeling","Bradypus","machine learning"],"title":"Talk: Using SVMs to model ranges of congeneric sloth species","type":"talk"},{"authors":["Cecina Babich Morrow"],"categories":null,"content":"\n","date":1550811600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550811600,"objectID":"11e2c6671bc23edbd280baf5f5b75766","permalink":"/talk/la_lgbt_careertalk_2019/","publishdate":"2019-02-22T00:00:00-05:00","relpermalink":"/talk/la_lgbt_careertalk_2019/","section":"talk","summary":"I gave this talk to young people at the Los Angeles LGBT Center's Youth Center as part of a career talk from people working in museums. I talked about my own career path and shared some tips and suggestions for succeeding at STEM careers, particularly at museums.","tags":["career","education","LGBT"],"title":"Talk: STEM careers in museums","type":"talk"},{"authors":null,"categories":["R","Python"],"content":"   The reticulate package logo.   Using Python in RMarkdown In order to write blog posts using Python code, I wanted to figure out a way to include Python code chunks in RMarkdowns. When you insert a code chunk in RMarkdown, you have the option of specifying the language of that chunk: the default is R, but you can also insert a Bash, SQL, Python, etc. code chunk.\nWhen I attempted to insert a Python code chunk and import libraries, however, I kept getting the error:\nError in py_run_string_impl(code, local, convert) : ImportError: No module named sklearn.cluster\nFrom running Python in Atom, I knew I had the sklearn.cluster module installed, so the problem must be in the connection between R and Python.\n reticulate The reticulate package in R (website here allows R to interact with Python. I installed the package from RStudio.\n# install.packages(\u0026quot;reticulate\u0026quot;) library(reticulate)  Changing Python versions Installing reticulate still didn’t allow me to knit the RMarkdown with a Python code chunk, however. I followed the instructions in this post by Pablo Franco to check the Python version that reticulate was using:\npy_discover_config() I ended up with the following output:\npython: /usr/bin/python libpython: /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/config/libpython2.7.dylib pythonhome: /System/Library/Frameworks/Python.framework/Versions/2.7:/System/Library/Frameworks/Python.framework/Versions/2.7 version: 2.7.10 (default, Aug 17 2018, 19:45:58) [GCC 4.2.1 Compatible Apple LLVM 10.0.0 (clang-1000.0.42)] numpy: /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy numpy_version: 1.8.0 I wanted to be running Python version 3.6, which was the version I had installed using Anaconda, so I needed to change the path.\nSet-up chunk I discovered that you can set the path to a different installation of Python by modifying the setup chunk at the start of the RMarkdown. According to the bookdown website, the default used is Python 2.\nMy default version of this set-up chunk looks like this:\n{r setup, include=FALSE} knitr::opts_chunk$set(collapse = TRUE) You can set the chunk option engine.path to specify the path to the engine interpreter and change it from the default Python 2.\n Finding Python path I now needed to find the actual path to Python that I wanted to use. I did this by opening up Python separately from RStudio (I used Atom for this) and running the following (I got the code for this from here):\nimport sys for p in sys.path: print(p) ## /Applications/anaconda3/bin ## /Applications/anaconda3/lib/python37.zip ## /Applications/anaconda3/lib/python3.7 ## /Applications/anaconda3/lib/python3.7/lib-dynload ## /Applications/anaconda3/lib/python3.7/site-packages ## /Applications/anaconda3/lib/python3.7/site-packages/aeosa ## /Library/Frameworks/R.framework/Versions/3.5/Resources/library/reticulate/python From this information, I could tell I wanted to use the path /anaconda3/lib/python3.6, rather than /usr/bin/python, which is what RMarkdown had originally been using. I modified by set-up chunk to look like this:\n{r setup, include=FALSE} knitr::opts_chunk$set(collapse = TRUE, engine.path = list(python = \u0026#39;/anaconda3/bin/python3.6\u0026#39;))   Other options This solution enabled me to knit RMarkdowns with Python code chunks! It changes the engine interpreter globally, which you could do for multiple engines simultaneously, like Python and Ruby, for example:\nknitr::opts_chunk$set(engine.path = list( python = \u0026#39;/anaconda3/bin/python3.6\u0026#39;, ruby = \u0026#39;/usr/local/bin/ruby\u0026#39; )) Alternatively, you can specify the engine interpreter locally in each code chunk by starting the chunk with {python, engine.path = '/anaconda3/bin/python3.6}, for example.\n ","date":1550715194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550715194,"objectID":"4e0a9da2f4c74d3a889ada7164b7f75d","permalink":"/post/2019-02-20-pythonrmd/","publishdate":"2019-02-20T21:13:14-05:00","relpermalink":"/post/2019-02-20-pythonrmd/","section":"post","summary":"The reticulate package logo.   Using Python in RMarkdown In order to write blog posts using Python code, I wanted to figure out a way to include Python code chunks in RMarkdowns. When you insert a code chunk in RMarkdown, you have the option of specifying the language of that chunk: the default is R, but you can also insert a Bash, SQL, Python, etc. code chunk.","tags":["R","Python","website"],"title":"Python in RMarkdown","type":"post"},{"authors":null,"categories":["Hackathon","Meteorites"],"content":"   Last weekend I had the opportunity to participate in my first ever hackathon: the #HackTheSolarSystem hackathon hosted by the American Museum of Natural History.\nWhat is a hackathon, anyways? This is a question I got asked countless times this last week as I informed my friends and family that I would be at work all weekend (yes, including Friday night, yes, including Saturday night, no, I will not be able to socialize at all). And, in fairness, this is a question I wouldn’t have had an answer to prior to this job. A hackathon is an event where participants come together to use technology to address a challenge. Usually, the event takes place over a few days, where hackers form teams and collaborate on their projects, before presenting their solutions.\nWhen I learned about this concept, I was baffled by the idea that it would be possible to come up with any kind of viable solution to a problem in only 24 hours. Coming from an academic background, I’m used to months of reading papers to get oriented to a problem before I can produce any kind of meaningful code. This event forced us to dive right in, looking at the problem from a computer science standpoint, rather than striving to get the full scientific background on the problem. I was amazed by the ability of all the teams at the event to create meaningful solutions to scientific problems they had not necessarily studied before.\n The AMMH Hackathon For the past five years, the American Museum of Natural History has hosted a hackathon. #HackTheSolarSystem is the first and only hackathon I have participated in thus far, so I can’t compare it to other events, but my understanding is that the museum seeks to put on a slightly different type of event than other hackathons: AMNH wants to give technologists and scientists the opportunity to work together to create solutions to real research problems. For #HackTheSolarSystem, museum scientists from the Department of Earth and Planetary Science put forth a series of challenges they face in their research that they thought could be solved through technology. These challenges ranged from tracking dust particles through aerogel to visualizing the Sun (a full list of challenges is available here). In addition to the scientific challenges, there were also educational challenges for both high school students and educators.\nOn Friday night, the museum “stakeholders”, i.e. the scientists posing the challenges, presented their challenges and led participants on tours of various areas of the museum, like the Hall of Meteorites and the Earth and Planetary Sciences department. Then, from Saturday at 2:00 pm to Sunday at 1:00 pm, us hackers stayed at the museum to develop solutions to our chosen challenges. Most hackers worked on teams formed at the event, although some corporate teams came into the hackathon with a team already formed.\n The Challenge I chose to work on the meteorite mineral mapping challenge: our “stakeholders”, museum scientists Marina Gemma and Sam Alpert, wanted a way to identify mineral composition in images of meteorites. Each meteorite slice had been imaged, resulting in data consisting of pixel intensity for a handful of elements. We also had access to a series of standards with known mineral content so we could compare the pixel intensities in these known minerals with the pixel intensities in the unknown minerals.\nOur team (pictured below) used four different approaches to address this problem: linear classification via SVM, random forest classification, nearest neighbor classification, and cluster inference. You can check out more details on these solutions in the Readme of our GitHub repository: https://github.com/HackTheSolarSystem/MineralMapping. (And I will post a more detailed explanation of the clustering solution in a later post, which is the part of the code I focused on!)\n  From left to right: Peter Kang, Jackson Lee, Jeremy Neiman, John Underwood, Katy Abbott, Cecina Babich Morrow, Meret Götschel   Our solution ended up making us one of the four primary winning teams at the hackathon! We won the “LabCoat Knockout” award for creating a solution with the potential to change the nature of research. We plan to continue working with our museum stakeholders to fine-tune our code so that it can be used in their research in the future. To me, that is the most exciting part of this event - that a group of technologists from diverse backgrounds (none of which knew much about meteorites before this event!) could write code in 24 hours with the potential to solve a scientific problem with a concrete impact on research at the museum.\n ","date":1550196794,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550196794,"objectID":"034f513a1f92c992514c53e4274fc878","permalink":"/post/2019-02-14-hackathon/","publishdate":"2019-02-14T21:13:14-05:00","relpermalink":"/post/2019-02-14-hackathon/","section":"post","summary":"Last weekend I had the opportunity to participate in my first ever hackathon: the #HackTheSolarSystem hackathon hosted by the American Museum of Natural History.\nWhat is a hackathon, anyways? This is a question I got asked countless times this last week as I informed my friends and family that I would be at work all weekend (yes, including Friday night, yes, including Saturday night, no, I will not be able to socialize at all).","tags":["hackathon","meteorites","earth science"],"title":"#HackTheSolarSystem","type":"post"},{"authors":null,"categories":["R","phylogenetics","evolution"],"content":"   Inspiration for this post This post comes from the finishing touches I needed to put on a paper about life history evolution. The paper compares life history traits across the four groups of tetrapods (amphibians, reptiles, mammals, and birds), so when I carried out the nitty gritty phylogenetic analyses, I used four separate phylogenies. When it came time to make a figure to visualize those analyses, however, I ended up with an unwieldy (and unpublishable) 16-panel figure (four phylogenies by four traits). One of my coauthors suggested using a tetrapod supertree to visualize the evolution of the traits across all four classes simultaneously.\nUyeda et al. 2017 Uyeda et al. did something similar in their 2017 paper The evolution of energetic scaling across the vertebrate tree of life. They stitched together fish, amphibian, squamate, bird, and mammal phylogenies together to visualize metabolic rate across all vertebrates:\n  Figure 1 from Uyeda et al. 2017   At first, I was very hopeful that I would be able to download this supertree and prune it to the taxa in my analysis since the authors were using the same clade specific phylogenies that I was. The phylogeny is available on Data Dryad (https://datadryad.org/resource/doi:10.5061/dryad.3c6d2). Unfortunately, after downloading that phylogeny and pruning it to include species I used in my analysis, I ended up with approximately 15% of the species I analyzed in the resulting tree.\n  Into the code Since my easy solution didn’t pan out and I couldn’t get enough information from the supplemental material for the paper to replicate their analyses, I looked on GitHub to try to find Uyeda’s code. Hooray for GitHub once again, because the repository for the paper can be found here: https://github.com/uyedaj/bmr. The RMarkdown details the analyses for the paper, including the process for making the full tree.\nStart with loading the necessary packages.\nlibrary(phytools) ## Loading required package: ape ## Warning: package \u0026#39;ape\u0026#39; was built under R version 3.5.2 ## Loading required package: maps library(geiger) The original phylogenies I was working with four separate phylogenies: amphibians, squamates, birds, and mammals. For amphibians, I used a congruified time-tree from the PhyloOrchard package (O’Meara et al. 2013) that was constructed using the Alfaro et al. timetree of gnathostomes (Alfaro et al. 2009) as the reference and the Pyron and Wiens amphibian phylogeny as the target (Pyron and Wiens 2011).\nlength(amphibiantree$tip.label) ## [1] 2871 plot(amphibiantree, type = \u0026quot;fan\u0026quot;, show.tip.label = FALSE) For squamates, I used the Zheng and Wiens time-calibrated phylogeny (Zheng and Wiens 2016).\nlength(squamatetree$tip.label) ## [1] 378 plot(squamatetree, type = \u0026quot;fan\u0026quot;, show.tip.label = FALSE) For birds, I used the Jetz phylogeny (Jetz et al. 2012).\nlength(birdtree$tip.label) ## [1] 9993 plot(birdtree, type = \u0026quot;fan\u0026quot;, show.tip.label = FALSE) For mammals, I used the supertree from Fritz et al. 2009.\nlength(mammaltree$tip.label) ## [1] 5020 plot(mammaltree, type = \u0026quot;fan\u0026quot;, show.tip.label = FALSE)  Reading in trees The first step of the process is reading in the individual phylogenies you want to stitch together. This step is straightforward, with one exception: you cannot have species that are present in multiple of the individual trees. For example, my squamate phylogeny included Gallus gallus (red junglefowl) and Dromaius novaehollandia (emu). Since these species were also present in my bird phylogeny, I got the following error: Found matching tips in 'subtree' and 'phy'. To solve this problem, I just removed these tips from the squamate tree:\nsquamatetree \u0026lt;- drop.tip(phy = squamatetree, tip = c(\u0026quot;Gallus_gallus\u0026quot;, \u0026quot;Dromaius_novaehollandiae\u0026quot;)) I ended up with the following list of trees and corresponding tip labels:\ntree_list \u0026lt;- list(amphib=amphibiantree, birds=birdtree, squam=squamatetree, mamm=mammaltree) class(tree_list) \u0026lt;- \u0026quot;multiPhylo\u0026quot;  Make a tree with orders In Uyeda et al. (2017), the authors were creating a phylogeny for all vertebrates, but for my analyses I was only examining tetrapods, so I didn’t have a fish phylogeny to include. The original code from Uyeda et al. to create a tree with the 5 vertebrate orders looks like this:\ntip.labels \u0026lt;- c(\u0026quot;fish\u0026quot;, \u0026quot;amphib\u0026quot;, \u0026quot;squam\u0026quot;, \u0026quot;birds\u0026quot;, \u0026quot;mamm\u0026quot;) ## Make a tree with just orders: edge \u0026lt;- matrix(c(9, 4, 9, 3, 8, 5, 8, 9, 7, 8, 7, 2, 6, 7, 6, 1), byrow=TRUE, ncol=2) ## Dates from Timetree of life (timetree.org) edge.length \u0026lt;- c(274.9, 274.9, 324.5, 324.5-274.9, 382.9-324.5, 382.9, 454.6-382.9 , 454.6) Nnode \u0026lt;- 4 ordertree \u0026lt;- list(edge=edge, Nnode=Nnode, tip.label=tip.labels, edge.length=edge.length) class(ordertree) \u0026lt;- \u0026#39;phylo\u0026#39; plot(ordertree) To visualize the results, we can add tip labels, node labels, and edge labels to the tree with the branch lengths:\nplot(ordertree) tiplabels() nodelabels() edgelabels(ordertree$edge.length, bg=\u0026quot;black\u0026quot;, col=\u0026quot;white\u0026quot;, font=2) Getting rid of fish Since I didn’t have fish, I needed to make a few modifications. First, tip.labels didn’t need “fish” in it anymore:\n# remove \u0026quot;fish\u0026quot; from tip.labels: tip.labels \u0026lt;- c(\u0026quot;amphib\u0026quot;, \u0026quot;squam\u0026quot;, \u0026quot;birds\u0026quot;, \u0026quot;mamm\u0026quot;) Now, for the trickier part - I needed to modify the edge matrix. The edge matrix contains the starting and ending nodes for each edge in a tree. As we can see from the plot above, numbering works in the following way: the tips are numbered starting at the top from 1 to the number of tips and the nodes are numbered starting at the root and moving towards the tips. To get rid of fish, I needed to delete one tip from the tree and one node (the original root node). I sketched out what I wanted the new order tree to look like, complete with numbered nodes and tips, and created the following edge matrix:\nedge \u0026lt;- matrix(c(7, 3, 7, 2, 6, 4, 6, 7, 5, 6, 5, 1), byrow=TRUE, ncol=2) Since I was losing two edges from the phylogeny (the one going from the root to fish and the one from the root to the last common ancestor of tetrapods), I also needed to modify the edge lengths by removing 454.6-382.9 and 454.6:\nedge.length \u0026lt;- c(274.9, 274.9, 324.5, 324.5-274.9, 382.9-324.5, 382.9) The final modification I needed was to decrease Nnode from 4 to 3:\nNnode \u0026lt;- 3 So now…\nordertree \u0026lt;- list(edge=edge, Nnode=Nnode, tip.label=tip.labels, edge.length=edge.length) class(ordertree) \u0026lt;- \u0026#39;phylo\u0026#39; plot(ordertree) edgelabels(ordertree$edge.length, bg=\u0026quot;black\u0026quot;, col=\u0026quot;white\u0026quot;, font=2) …I was ready to go with an order-level tree onto which I could graft my individual phylogenies!\n Node dates …Not so fast. I ended up with an additional problem I needed to solve before grafting the trees together. I ran into the error 'split_age' is inconsistent with edge lengths in 'phy', which means that the earliest node in one of my individual phylogenies was older than the node age I gave in edge.length. By using debug, I was able to tell that the error occurred when I added the squamate tree. The oldest node in my squamate tree was 277.8 million years ago, but I had set the divergence time between birds and squamates at 274.9 mya, so R was having problems. The species causing the problem was the tuatara, which is the only surviving member of its order.\n  The pesky (yet very cute) tuatara (https://www.australiangeographic.com.au/blogs/creatura-blog/2017/12/the-tuatara/)   I had a couple of choices: either delete the tuatara from the squamate phylogeny or increase the age of the last common ancestor of birds and squamates when I created the vector edge.length. I chose to do the latter because why get rid of such a cool animal!\nI went to http://timetree.org/ to see if I could find a reasonable range of estimates for this node. According to the website, which allows you to search for the divergence time between any two taxa, the estimated divergence of birds and squamates occurred 280 mya.\n  TimeTree results for divergence time of birds and squamates.   So I ended up with the following code and order tree:\ntip.labels \u0026lt;- c(\u0026quot;amphib\u0026quot;, \u0026quot;squam\u0026quot;, \u0026quot;birds\u0026quot;, \u0026quot;mamm\u0026quot;) edge \u0026lt;- matrix(c(7, 3, 7, 2, 6, 4, 6, 7, 5, 6, 5, 1), byrow=TRUE, ncol=2) edge.length \u0026lt;- c(280, 280, 324.5, 324.5-274.9, 382.9-324.5, 382.9) Nnode \u0026lt;- 3 ordertree \u0026lt;- list(edge=edge, Nnode=Nnode, tip.label=tip.labels, edge.length=edge.length) class(ordertree) \u0026lt;- \u0026#39;phylo\u0026#39; plot(ordertree) edgelabels(ordertree$edge.length, bg=\u0026quot;black\u0026quot;, col=\u0026quot;white\u0026quot;, font=2) tree_list \u0026lt;- list(amphib=amphibiantree, birds=birdtree, squam=squamatetree, mamm=mammaltree) class(tree_list) \u0026lt;- \u0026quot;multiPhylo\u0026quot;   Grafting the trees The final step is grafting the individual trees onto the order tree in the proper place.\n#Add taxonomic information to tree otax \u0026lt;- data.frame(\u0026quot;Class\u0026quot;= ordertree$tip.label, \u0026quot;Superclass\u0026quot;=c(rep(\u0026quot;Tetrapoda\u0026quot;,2))) rownames(otax) \u0026lt;- ordertree$tip.label classtree \u0026lt;- nodelabel.phylo(ordertree, otax, ncores=1) res \u0026lt;- glomogram.phylo(classtree, tree_list) plot(res, type = \u0026quot;fan\u0026quot;, show.tip.label = FALSE) Voila - a tree with 18262 species of tetrapods!\n  Disclaimer I’d like to finish this post with a disclaimer: I am NOT a phylogeneticist (yet?). The supertree created in this analysis incorporates several different phylogenies from literature and adapts code from another published article (all written by people with much more phylogenetic background than I!). However, the accuracy of the tree decreases as you move back in time - there is a great deal of uncertainty about node age for the deeper nodes in the tree. Even so, this process allows us to make some cool visualizations to compare major clades across vast stretches of evolutionary time - even if precise dates are incorrect, overall patterns are still informative!\n Code The entire script I used for this process can be found at https://github.com/KerkhoffLab/bodymasspatterns/blob/master/tetrapod_phylogeny_code.R.\n Literature Cited Alfaro, M. E., F. Santini, C. Brock, H. Alamillo, A. Dornburg, D. L. Rabosky, G. Carnevale, and L. J. Harmon. 2009. Nine exceptional radiations plus high turnover explain species diversity in jawed vertebrates. PNAS 106:13410-13414.\nFritz, S. A., O. R. P. Bininda-Emonds, and A. Purvis. 2009. Geographical variation in predictors of mammalian extinction risk: big is bad, but only in the tropics. Ecology Letters 12:538–549.\nJetz, W., G. H. Thomas, J. B. Joy, K. Hartmann, and A. O. Mooers. 2012. The global diversity of birds in space and time. Nature 491:444.\nO’Meara, B. C., L. J. Harmon, and J. Eastman. 2013. PhyloOrchard: Important and/or useful phylogenetic datasets.\nPyron, R. A. and J. J. Wiens. 2011. A large-scale phylogeny of Amphibia including over 2800 species, and a revised classification of extant frogs, salamanders, and caecilians. Molecular Phylogenetics and Evolution 61: 543-583.\nUyeda JC, Pennell MW, Miller ET, Maia R, McClain CR (2017) The evolution of energetic scaling across the vertebrate tree of life. The American Naturalist 190(2): 185-199. https://doi.org/10.1086/692326\nUyeda JC, Pennell MW, Miller ET, Maia R, McClain CR (2017) Data from: The evolution of energetic scaling across the vertebrate tree of life. Dryad Digital Repository. https://doi.org/10.5061/dryad.3c6d2\nZheng, Y., and J. J. Wiens. 2016. Combining phylogenomic and supermatrix approaches, and a time-calibrated phylogeny for squamate reptiles (lizards and snakes) based on 52 genes and 4162 species. Molecular Phylogenetics and Evolution 94:537–547.\n ","date":1547863994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547863994,"objectID":"37de5ace87653c09aba9c603d012c0a0","permalink":"/post/2019-01-09-grafting-trees/","publishdate":"2019-01-18T21:13:14-05:00","relpermalink":"/post/2019-01-09-grafting-trees/","section":"post","summary":"Inspiration for this post This post comes from the finishing touches I needed to put on a paper about life history evolution. The paper compares life history traits across the four groups of tetrapods (amphibians, reptiles, mammals, and birds), so when I carried out the nitty gritty phylogenetic analyses, I used four separate phylogenies. When it came time to make a figure to visualize those analyses, however, I ended up with an unwieldy (and unpublishable) 16-panel figure (four phylogenies by four traits).","tags":["R","phylogenetics","evolution"],"title":"Grafting phylogenies","type":"post"},{"authors":["Cecina Babich Morrow","Andrew J. Kerkhoff","S.K. Morgan Ernest"],"categories":null,"content":"","date":1547614800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547614800,"objectID":"477dac710ad42eb4e52c835c6e27ae11","permalink":"/publication/lifehistory/","publishdate":"2019-01-16T00:00:00-05:00","relpermalink":"/publication/lifehistory/","section":"publication","summary":"Life history traits represent organism's strategies to navigate the fitness trade-offs between survival and reproduction. Eric Charnov developed three dimensionless metrics to quantify fundamental life history trade-offs. Lifetime reproductive effort (LRE), relative reproductive lifespan (RRL), and relative offspring size (ROS), together with body mass, can be used classify life history strategies across the four major classes of tetrapods: amphibians, reptiles, mammals, and birds. First, we investigate how the metrics have evolved in concert with body mass. In most cases, we find evidence for correlated evolution between body mass and the three metrics. Finally, we compare life history strategies across the four classes of tetrapods and find that LRE, RRL, and ROS delineate a space in which the major tetrapod clades occupy mostly unique subspaces. These distinct combinations of life history strategies provide us with a framework to understand the impact of major evolutionary transitions in energetics, physiology, and ecology.","tags":["hypervolume","life history"],"title":"PREPRINT: Macroevolution of dimensionless life history metrics in tetrapods","type":"publication"},{"authors":null,"categories":["R"],"content":"   PhD comics Jorge Cham (www.phdcomics.com)   Recently I’ve been part of beta testing a new R package. The package allows users to apply different masks to spcies distribution models to create more ecologically realistic range models. The package itself is very cool, but since it hasn’t been released yet, this post will be short on ecology. Instead, I’m going to focus on the process I’ve been using to debug the package. This debugging thought process and the different functions that make it possible are things I wish I’d been formally taught while learning R (but better late than never!).\ntraceback The traceback() function is definitely one of those that I wished I learned sooner. Basically, traceback() shows you the series of functions called, including the one resulting in the last error. For example, when I ran traceback() after the function I was testing threw an error, I got the following readout:\n  When I tried to use the function rangeSVM(), I got an error, as we can see in the first line of code I ran. By running traceback(), I can see the order of functions that rangeSVM() uses: first, it called the svm() function from the R package e1071, which in turn called svm.formula(), which finally called svm.default(). This last function, svm.default() is what produced the error (as we can see from the error message, as well).\nRunning traceback() can be especially helpful in situations like this, where the error message comes from a function called internally by the function you actually ran. Here, the error does not come from the rangeSVM() function, so reading the documentation for rangeSVM() could not save me. Now I know that the error actually comes from a function in the e1071 package, so I could check out the source code to figure out what kind of issue causes that error message.\n Checking source code There is a CRAN GitHub account that is an unofficial read-only mirror of all CRAN packages. Typically, when I want to read through the code for a particular function in a package, this is what I use. To dig a little deeper into what was going wrong with the svm() function, I found the code here. This allowed me to look at the function definitions for svm.formula() as well as svm.default().\n debug Reading through the source code is helpful, but wouldn’t it be even better if you could watch R step through the function line by line until the error? Fortunately the handy debug() function does exactly that. By running the following:\ndebug(rangeSVM) svmHYB_weight \u0026lt;- rangeSVM(variegatus[,2:3], tridactylus[,2:3], sdm = raster::stack(var_sdm, tri_sdm), nrep = 3, weight = TRUE) I could see that the error occurred at line 100 of svm.R:\n# Browse[2]\u0026gt; # debug at /Users/hellenfellows/OneDrive - AMNH/Wallace/maskRangerBetaTesting/maskRangeR/R/svm.R#100: m \u0026lt;- e1071::svm(sp ~ ., data = xy, gamma = params_best_df_mostFreq$gamma[1], # cost = params_best_df_mostFreq$cost[1], class.weights = cw) # Browse[2]\u0026gt; # Error in svm.default(x, y, scale = scale, ..., na.action = na.action) : # NA/NaN/Inf in foreign function call (arg 10) The Browse\u0026gt; prompt at the beginning of the lines indicate that the debugger is working. The debug at part shows the next line of code to be executed when you hit enter next. R will continue to show you the following line of code as you press enter (up until you hit the error). If you are running all of this in RStudio, it will also conveniently highlight the lines in the source code that will be run next. Also, when you want to get out of the debugger so you can run that function again later without debugging, run undebug(rangeSVM), for example.\nMy initial suspicion was that the function was failing during one of the runs of a for loop, however the debugger showed me that the function was making it through all the runs of the for loop. Instead, the error occurred during the final step of rangeSVM() when the function attempted to integrate the results of all runs of the for loop to create a final support vector machine model.\nget To figure out why this problem was occurring, I wanted to be able to see the values of the different variables passeed to that final step of the function. Handily enough, while in the debugger, you can use the function get(\u0026quot;variable\u0026quot;) to check the status of the different variables (the name of the variable needs to be in quotation marks).\nFor example:\ndebug(rangeSVM) svmHYB \u0026lt;- rangeSVM(variegatus[,2:3], tridactylus[,2:3], sdm = raster::stack(var_sdm, tri_sdm), nrep = 3) # Browse[2]\u0026gt; # debug at /Users/hellenfellows/OneDrive - AMNH/Wallace/maskRangerBetaTesting/maskRangeR/R/svm.R#96: params_best_df$params \u0026lt;- paste0(params_best_df$gamma, params_best_df$cost) # Browse[2]\u0026gt; get(\u0026quot;params_best_df\u0026quot;) # gamma cost class.weights # 38 0.5000000 2 1 # 85 0.0078125 2048 1 # 95 0.0078125 8192 1 This was the final step of the process that actually allowed me to figure out the problem:\n# Browse[2]\u0026gt; get(\u0026quot;params_best_df_mostFreq\u0026quot;) # [1] gamma cost # \u0026lt;0 rows\u0026gt; (or 0-length row.names) Without going into too much detail about the function itself, I was able to tell that the parameters I was feeding into the support vector machine function somehow didn’t exist: the variable params_best_df_mostFreq had 0 rows.\n  Other tips and tricks I got much of these resources from a very helpful blog post called “Tracking down errors in R” by Pete Werner (also available as a post on R-bloggers). This post goes through a slightly simpler example of debugging which is reproducible (something I have not attempted to do here), so it is very helpful for seeing the process on a simpler function. In addition to the techniques I used, Pete also explains how you can turn warning messages into errors in the case that your function is throwing a troubling warning message that you suspect is causing an error later on.\n ","date":1546567994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546567994,"objectID":"91ff7b88c34bcad34a7aa89e491b6a42","permalink":"/post/2019-01-03-debugging-r/","publishdate":"2019-01-03T21:13:14-05:00","relpermalink":"/post/2019-01-03-debugging-r/","section":"post","summary":"PhD comics Jorge Cham (www.phdcomics.com)   Recently I’ve been part of beta testing a new R package. The package allows users to apply different masks to spcies distribution models to create more ecologically realistic range models. The package itself is very cool, but since it hasn’t been released yet, this post will be short on ecology. Instead, I’m going to focus on the process I’ve been using to debug the package.","tags":["R"],"title":"Debugging in R","type":"post"},{"authors":null,"categories":["R","GitHub"],"content":" Given that the Academic website theme comes with a place to upload talks, I thought I might as well upload the slides for my Senior Honors presentation. Unfortunately, this was a slightly more difficult task than I anticipated. The default format for slides in Hugo appears to be Markdown slides, which is fine, except when your talk was made in Google Slides.\nGoogle Slides to Markdown? My first thought was that there must be some tool to convert a Google slides talk into a Markdown document. For whatever reason, there are several tools (like this GitHub repo) to go the other direction, from Markdown to Google slides. I found a GitHub repo gdocs2md that claimed to go in the other direction, but the script provided doesn’t currently run (if anyone is a whiz at Google Apps and wants to take a stab at fixing up this code, it would be a great tool!). There is a Google Drive add-on called GD2md-html that converts Google Docs to Markdown or HTML, which should be useful for the future, but it doesn’t help with Google Slides.\n Embedding Google Slides in Hugo Having abandoned the idea of just converting my current slides into Markdown and going on my merry way, I started looking for a way to embed the Google Slides themselves into my website.\nHugo shortcodes Hugo has certain snippets called shortcodes that are designed to address the problem of having to add raw HTML into Markdown. These shortcodes help render a content file according to a template created by Hugo. For example, the shortcode figure adds HTML functionality to the Markdown image syntax. There are shortcodes for Instagram (instagram), Vimeo (vimeo) and Twitter (tweet).\n The gdocs shortcode One of these shortcodes, gdocs, allows you to embed all Google doc types (Slides, Docs, Sheets, etc.) into your website. The HTML code for the shortcode can be found in the hugo-academic GitHub repo: https://github.com/gcushen/hugo-academic/blob/master/layouts/shortcodes/gdocs.html.\nI tried inserting the following into the index.md file for my talk below the line with three + signs:\n{{\u0026lt; gdocs \u0026quot;https://docs.google.com/presentation/d/e/2PACX-1vTsSj-oftLksGEKTkzCoBSIpzooO61ZMGAUzoGBv7u20OzMQK8ctX3gCpWTMm4UowrrWumFrdQsY0Cd/embed?start=false\u0026amp;loop=false\u0026amp;delayms=5000\u0026quot; \u0026gt;}} I got the URL by going to my Google Slides, and clicking File \u0026gt; Publish to the web. I selected Embed and copied the link following src=.\nUnfortunately, this didn’t result in anything showing up on my page besides a large blank space where I assume Hugo was trying to put my presentation. I tried some variations on the above code, including adding src= before the URL (no change), and moving the entire code snippet above the +++ line in index.md (results in the error \u0026quot; unmarshal failed: Near line 65 (last key parsed ‘image’): bare keys cannot contain ‘{’ “).\n HTML code At this point, I was feeling pretty discouraged, when I happened upon a Hugo forum post with the HTML code someone used to embed a Google Slides presentation. I have hardly any familiarity with HTML, but clearly shortcodes and I weren’t getting along very well, so I decided to give it a shot. I copied and pasted the code from the forum, replacing the URL following src= with my link from above, and voila! My slides appeared on the post.\nHere is the code for reference:\n\u0026lt;p\u0026gt;\u0026lt;iframe src=\u0026quot;https://docs.google.com/presentation/d/e/2PACX-1vTsSj-oftLksGEKTkzCoBSIpzooO61ZMGAUzoGBv7u20OzMQK8ctX3gCpWTMm4UowrrWumFrdQsY0Cd/embed?start=false\u0026amp;loop=false\u0026amp;delayms=5000\u0026quot; frameborder=\u0026quot;0\u0026quot; width=\u0026quot;800\u0026quot; height=\u0026quot;600\u0026quot; allowfullscreen=\u0026quot;true\u0026quot; mozallowfullscreen=\u0026quot;true\u0026quot; webkitallowfullscreen=\u0026quot;true\u0026quot;\u0026gt;\u0026lt;/iframe\u0026gt;\u0026lt;/p\u0026gt;   Moral of the story It seems like shortcodes can be a really valuable feature of Hugo, especially should I ever need to include a video of some kind (or if I actually start using Twitter again and tweet something worth sharing, two improbable events). In the case of Google Docs/Sheets/etc., however, just using the HTML code snippet seems like the easiest option (and is a good motivation to learn HTML at a later date).\n ","date":1544667194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544667194,"objectID":"fcb45bed5417a023f0e46ff8490a4891","permalink":"/post/2018-12-12-add-google-doc/","publishdate":"2018-12-12T21:13:14-05:00","relpermalink":"/post/2018-12-12-add-google-doc/","section":"post","summary":"Given that the Academic website theme comes with a place to upload talks, I thought I might as well upload the slides for my Senior Honors presentation. Unfortunately, this was a slightly more difficult task than I anticipated. The default format for slides in Hugo appears to be Markdown slides, which is fine, except when your talk was made in Google Slides.\nGoogle Slides to Markdown? My first thought was that there must be some tool to convert a Google slides talk into a Markdown document.","tags":["R","GitHub","website"],"title":"Adding Google Docs to website","type":"post"},{"authors":null,"categories":["R","GitHub"],"content":" As I continue setting up this site, one of the major things irking me was the order of content on the website. For example, I wasn’t loving that the first thing you saw when visiting the site was the giant picture of my face. I wanted to be able to put content at the top of the site and move information like my bio and work experience towards the bottom. Hugo allows you to weight certain types of content to change the order, so that’s what I tackled.\nMenu links The first order of things I could change was the order of links in the menu on the top right of my page. In this case, I wanted my information to be the first link, followed by “Publications”, “Projects”, “Posts”, “Tutorials”, and “Contact”. The weighting for these links is found in the config.toml file of the website repo, where a standard entry for one of the links looks a bit like this:\n[[menu.main]] name = \u0026quot;Publications\u0026quot; url = \u0026quot;#publications\u0026quot; weight = 2 Here you can change the “name”, which is what the menu option actually says (e.g. I changed the menu option for my bio from “Home” to “About”). The URL refers to where on the site the link will point to. If you are trying to link to a homepage widget, as in the example above, the url will be # followed by the file name of that widget in the content/home/ folder. You want to be careful not to link to a widget that is inactive (which you can see in the first few lines of the Markdown document for that widget), otherwise the link will not go anywhere.\nIf you are linking to something else, you can provide the file-path relative to the content folder – for example, my tutorials live in the folder content/tutorial:\n[[menu.main]] name = \u0026quot;Tutorials\u0026quot; url = \u0026quot;/tutorial/\u0026quot; weight = 5 Finally, you can change the weights to reflect the order you want the links to appear in the menu: the links will appear from left to right in ascending order of weight (weight = 1 is the first link, etc.)\n Home page content Initially, I confused changing the weights in the config.toml file with actually changing the order of those sections on the home page, but doing that actually requires editing the Markdown files for those types of content in the content/home/ folder. For example, I decided I wanted to put posts first, so I went to content/home/posts.md and modified the weight argument so that it was lowest and continued editing the weights until everything was in the order I wanted.\n ","date":1544580794,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544580794,"objectID":"a6892cf666663b12c2f001ba2287bc31","permalink":"/post/2018-12-11-order-items/","publishdate":"2018-12-11T21:13:14-05:00","relpermalink":"/post/2018-12-11-order-items/","section":"post","summary":"As I continue setting up this site, one of the major things irking me was the order of content on the website. For example, I wasn’t loving that the first thing you saw when visiting the site was the giant picture of my face. I wanted to be able to put content at the top of the site and move information like my bio and work experience towards the bottom.","tags":["R","GitHub","website"],"title":"Ordering website components","type":"post"},{"authors":null,"categories":["R","GitHub"],"content":" Getting started I’ve been contemplating the idea of creating a website for sometime, but my initial (mostly half-hearted) attempts ran into some serious technological glitches. I started out trying to create a GitHub site using Jekyll, since I knew I wanted my website to be hosted via GitHub to keep all of my code in the same place. After about an hour of unsuccessful commits that resulted in a lovely white page with my name on it, I scrapped the whole venture until I heard about the blogdown R package. Since R is my programming language of choice, the idea of being able to write posts in RMarkdown and preview my site from the comfort of RStudio was tremendously appealing. I saw a research site created through Hugo’s Academic theme using blogdown that referenced some helpful tutorials and decided to give the process another shot.\nHugo + blogdown After my initial failures with Jekyll, I knew enough about the process (and my own skills) to be wary of the Academic theme’s optimistic tagline “Create a beautifully simple site in under 10 minutes”. So fair warning: getting this site up and semi-running was the work of an afternoon plus. I am not going to provide a full tutorial here, since several excellent ones already exist, but I will give my input on which resources worked for me and where I had difficulties.\n  Useful resources I started out following Amber Thomas’s instructions for using blogdown, Hugo, and GitHub pages to create a website. I probably should have taken better note of her initial disclaimer that simpler methods of creating a site exist, but nonetheless I found her instructions useful for wrapping my head around the method behind the whole process. As she explains, GitHub pages and Hugo work in different ways: GitHub pages uses information from the master branch of the repository to build your website, while Hugo builds a website inside the public folder. The solution she presents in her tutorial creates a sub branch within GitHub. She provides some shell scripts written by Jente Hidskes that create the sub branch and also update the site. These did not run for me for whatever reason – I believe Hidskes has written an updated tutorial to fix them, however.\nWhile I was reading the comments on Thomas’s tutorials to try to troubleshoot the shell script problems, I found a different tutorial by Robert McDonnell that promised a more streamlined approach. McDonnell gets around the GitHub/Hugo conflicts by creating two GitHub repositories, one for the website (for example, my repo is called babichmorrowc.github.io) and one for all of the Hugo and blogdown content (mine is called website-hugo).\nTo link these two repositories, Thomas has users create a Git Submodule using the following code:\ngit submodule add -b master git@github.com:\u0026lt;username\u0026gt;/\u0026lt;username\u0026gt;.github.io.git public This is where, once again, I ran into difficulties. I kept getting the error:\nCloning into \u0026#39;\u0026lt;filepath\u0026gt;/website-hugo/public\u0026#39;... git@github.com: Permission denied (publickey). fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. fatal: clone of \u0026#39;git@github.com:babichmorrowc/babichmorrowc.github.io.git\u0026#39; into submodule path \u0026#39;\u0026lt;filepath\u0026gt;/website-hugo/public\u0026#39; failed  Fortunately, there turned out to be an easy fix for this – the code McDonnell provides uses the SSH URL, which is something I do not have set up. I replaced it with the HTTPS URL (code below), and went on my merry way.\ngit submodule add -b master https://github.com/babichmorrowc/babichmorrowc.github.io.git public  Pushing to GitHub From that point forward, I had a functioning site that I could preview in RStudio using the serve_site() function. Using the following Git commands, I could push those changes and get that same site up and running online:\n# From the folder for the repo website-hugo cd public git add . git commit -m \u0026quot;new commit!\u0026quot; git push origin master cd .. git add . git commit -m \u0026quot;new commit part 2\u0026quot; git push  Editing my site I’m still learning my way around the format of the Hugo site, but by some trial and error I’ve started the process of customizing the site. I am using the Academic theme. So far I’ve managed to update my bio, add a publication, and take a stab at creating projects. As I get more familiarity with the format, I will try for a more comprehensive blog post explaining how to add your own content.\n ","date":1544235194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544235194,"objectID":"54d7d81ed67058a3387b928edd138a51","permalink":"/post/2018-12-7-create-website/","publishdate":"2018-12-07T21:13:14-05:00","relpermalink":"/post/2018-12-7-create-website/","section":"post","summary":"Getting started I’ve been contemplating the idea of creating a website for sometime, but my initial (mostly half-hearted) attempts ran into some serious technological glitches. I started out trying to create a GitHub site using Jekyll, since I knew I wanted my website to be hosted via GitHub to keep all of my code in the same place. After about an hour of unsuccessful commits that resulted in a lovely white page with my name on it, I scrapped the whole venture until I heard about the blogdown R package.","tags":["R","GitHub","website"],"title":"Creating this website","type":"post"},{"authors":null,"categories":null,"content":"","date":1538020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538020800,"objectID":"f5086c62b7b314f1130ea115597b8650","permalink":"/project/bradypus/","publishdate":"2018-09-27T00:00:00-04:00","relpermalink":"/project/bradypus/","section":"project","summary":"Species distribution modelling of *Bradypus variegatus*, *B. tridactylus*, and *B. torquatus*.","tags":["species distribution modeling","Bradypus","biogeography"],"title":"Distribution modeling of Bradypus","type":"project"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536465600,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":["Cecina Babich Morrow","Drew Kerkhoff","Morgan Ernest"],"categories":null,"content":"\n","date":1525147200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525147200,"objectID":"43876c519de1dcde73da535526722534","permalink":"/talk/honors_presentation_2018/","publishdate":"2018-05-01T00:00:00-04:00","relpermalink":"/talk/honors_presentation_2018/","section":"talk","summary":"Organisms’ life history traits quantify their fitness strategies to navigate various energy allocation trade-offs between growth and reproduction. Eric Charnov presented three dimensionless metrics life history metrics that represent key life history trade-offs: lifetime reproductive effort (LRE), relative reproductive lifespan (RRL), and relative offspring size (ROS). These three metrics are theorized to be invariant with body mass, thus enabling comparisons of organisms across the body mass continuum. We use these metrics, in addition to body mass, to classify life history strategies across the four major classes of tetrapods: amphibians, reptiles, mammals, and birds. First, we assess body mass invariance of LRE, RRL, and ROS in 113 amphibians, 491 reptiles, 843 mammals, and 171 birds. We find that, although the metrics are not consistently invariant across classes, all three exhibit qualities of invariance in at least one class and LRE and RRL display invariant behavior in the majority of tetrapod classes. Second, we investigate how the traits have evolved in concert with body mass, either to maintain or disrupt the patterns of invariance we observe. Finally, we compare the values of these metrics and their combinations across the four classes of tetrapods. LRE, RRL, and ROS delineate a space in which the major tetrapod clades occupy mostly unique subspaces. These distinctions in combinations of life history strategies provide us with a framework to understand the impact of major evolutionary transitions in energetics, physiology, and ecology.","tags":["life history","hypervolume"],"title":"Talk: Macroevolution of dimensionless life history metrics in tetrapods","type":"talk"},{"authors":["Cecina Babich Morrow","Drew Kerkhoff","Morgan Ernest"],"categories":null,"content":"","date":1504238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504238400,"objectID":"d8b7bc75b9b20c60ef4e61e32564c0a5","permalink":"/talk/tree_2017/","publishdate":"2017-09-01T00:00:00-04:00","relpermalink":"/talk/tree_2017/","section":"talk","summary":"**Objectives:** 1. Use Charnov’s dimensionless life history traits to visualize and quantify the life history strategies of amniotes. 2. Compare life history strategies of birds, mammals, reptiles, and smaller clades by using hypervolumes. 3. Investigate if these so-called invariant traits are actually invariant with body mass. 4. Analyze the macroevolutionary patterns of the dimensionless traits and their components between clades.","tags":["life history","hypervolume"],"title":"Poster: Macroevolution of dimensionless life histories in amniotes","type":"talk"},{"authors":["Benjamin Blonder","Cecina Babich Morrow","Brian Maitner","David J. Harris","Christine Lamanna","Cyrille Violle","Brian J. Enquist","Andrew J. Kerkhoff"],"categories":null,"content":"","date":1502251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1502251200,"objectID":"4c4838d5dcbbecfd6ac05025d63e5798","permalink":"/publication/hypervolumes/","publishdate":"2017-08-09T00:00:00-04:00","relpermalink":"/publication/hypervolumes/","section":"publication","summary":"Hutchinson's n‐dimensional hypervolume concept underlies many applications in contemporary ecology and evolutionary biology. Estimating hypervolumes from sampled data has been an ongoing challenge due to conceptual and computational issues. We present new algorithms for delineating the boundaries and probability density within n‐dimensional hypervolumes. The methods produce smooth boundaries that can fit data either more loosely (Gaussian kernel density estimation) or more tightly (one‐classification via support vector machine). Further, the algorithms can accept abundance‐weighted data, and the resulting hypervolumes can be given a probabilistic interpretation and projected into geographic space. We demonstrate the properties of these methods on a large dataset that characterises the functional traits and geographic distribution of thousands of plants. The methods are available in version ≥2.0.7 of the hypervolume r package. These new algorithms provide: (i) a more robust approach for delineating the shape and density of n‐dimensional hypervolumes; (ii) more efficient performance on large and high‐dimensional datasets; and (iii) improved measures of functional diversity and environmental niche breadth.","tags":["hypervolume"],"title":"New approaches for delineating n‐dimensional hypervolumes","type":"publication"},{"authors":["Cecina Babich Morrow","Benjamin Blonder","Brian Maitner","Brian Enquist","Andrew Kerkhoff"],"categories":null,"content":"This poster was made using version 1.4.6 of the hypervolume R package. These results should not be considered representative of the current algorithms in the package - to see current algorithm performance, refer to Blonder et al. 2017.\n","date":1483246800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483246800,"objectID":"23316e16b370eca9fcffd7932f367047","permalink":"/talk/ibs_2017/","publishdate":"2017-01-01T00:00:00-05:00","relpermalink":"/talk/ibs_2017/","section":"talk","summary":"Multidimensional hypervolumes enable ecologists to visualize the functional trait space occupied by an ecological community. Previously, hypervolumes have been measured using a minimum convex hull, but convex hulls are exclusively determined by extreme points and they cannot account for possible holes in the trait space. A multivariate kernel density estimation method with hyperbox kernels was proposed to deal with high-dimensional or holey datasets, but this method produces unrealistically blocky hypervolumes. We examined two alternatives: a Gaussian kernel density estimation method and a support vector machine method. We tested these two new methods and the hyperbox method by creating hypervolumes for three New World biomes using trait data from plants and mammals. We varied the parameters for each method in order to determine sensitivity to parameter variation. The resulting hypervolumes were compared with respect to their total volume, shape, and overlap. The hyperbox hypervolumes consistently had the largest volume of the three methods. The Gaussian method proved least sensitive to variation in bandwidth, while the support vector machine is the most customizable in terms of its two parameters, but may be susceptible to overfitting.","tags":["plant biodiversity","BIEN","hypervolume"],"title":"Poster: Quantifying species tolerances and functional diversity using n-dimensional hypervolumes: a comparison of methods","type":"talk"},{"authors":null,"categories":null,"content":"","date":1469592000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1469592000,"objectID":"522d95df7835b342554012eb6e68d074","permalink":"/project/hypervolumes/","publishdate":"2016-07-27T00:00:00-04:00","relpermalink":"/project/hypervolumes/","section":"project","summary":"Testing new algorithms for creating *n*-dimensional hypervolumes.","tags":["hypervolume"],"title":"Hypervolume package testing","type":"project"},{"authors":null,"categories":null,"content":"This is a \u0026ldquo;hello world\u0026rdquo; example website for the blogdown package. The theme was forked from @jrutheiser/hugo-lithium-theme and modified by Yihui Xie.\n","date":1462510131,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462510131,"objectID":"6083a88ee3411b0d17ce02d738f69d47","permalink":"/about/","publishdate":"2016-05-05T21:48:51-07:00","relpermalink":"/about/","section":"","summary":"This is a \u0026ldquo;hello world\u0026rdquo; example website for the blogdown package. The theme was forked from @jrutheiser/hugo-lithium-theme and modified by Yihui Xie.","tags":null,"title":"About","type":"page"},{"authors":null,"categories":null,"content":"","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461729600,"objectID":"464da018524b5882ed21299c5517a1cf","permalink":"/project/life_history/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/project/life_history/","section":"project","summary":"Analyzing the evolution of dimensionless life history metrics across tetrapods.","tags":["life history","hypervolume"],"title":"Tetrapod life history traits","type":"project"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]